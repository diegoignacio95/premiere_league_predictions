{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match Stats Scraper - Production Scale\n",
    "\n",
    "This notebook contains clean, scalable functions to scrape match statistics from FBRef for multiple matches efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from typing import Optional, List, Dict, Tuple\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Headers to appear more like a regular browser\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(url: str, delay_range: Tuple[float, float] = (2, 4)) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Fetch page with error handling and rate limiting\n",
    "    \n",
    "    Args:\n",
    "        url: URL to fetch\n",
    "        delay_range: Tuple of (min_delay, max_delay) in seconds\n",
    "    \n",
    "    Returns:\n",
    "        BeautifulSoup object or None if failed\n",
    "    \"\"\"\n",
    "    time.sleep(random.uniform(*delay_range))  # Be respectful - random delay\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return BeautifulSoup(response.content, 'html.parser')\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_percentage_or_value(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract percentage first, if not found then extract first number\n",
    "    Prioritizes percentage values over other numbers\n",
    "    \"\"\"\n",
    "    # First try to find percentage\n",
    "    percentage_match = re.search(r'(\\d+(?:\\.\\d+)?%)', text)\n",
    "    if percentage_match:\n",
    "        return percentage_match.group(1)\n",
    "    \n",
    "    # If no percentage, try to find any number\n",
    "    number_match = re.search(r'(\\d+(?:\\.\\d+)?)', text)\n",
    "    if number_match:\n",
    "        return number_match.group(1)\n",
    "    \n",
    "    # If nothing found, return original text\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fixtures_from_json(json_filename: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load fixtures data from JSON file and convert to DataFrame\n",
    "    \n",
    "    Args:\n",
    "        json_filename: Path to JSON file\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with fixtures data\n",
    "    \"\"\"\n",
    "    with open(json_filename, 'r', encoding='utf-8') as f:\n",
    "        fixtures_data = json.load(f)\n",
    "    \n",
    "    return fixtures_data_to_dataframe(fixtures_data)\n",
    "\n",
    "def fixtures_data_to_dataframe(fixtures_data: Dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Convert fixtures data dictionary to a pandas DataFrame\n",
    "    \n",
    "    Args:\n",
    "        fixtures_data: Fixtures data from extract_all_team_fixtures()\n",
    "    \n",
    "    Returns:\n",
    "        Flattened DataFrame with one row per match\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    \n",
    "    for team_id, team_data in fixtures_data.items():\n",
    "        team_name = team_data['team_name']\n",
    "        \n",
    "        for season, season_data in team_data['seasons_data'].items():\n",
    "            if season_data and season_data.get('matches'):\n",
    "                \n",
    "                for match in season_data['matches']:\n",
    "                    # Create a record for each match\n",
    "                    record = {\n",
    "                        'team_id': team_id,\n",
    "                        'team_name': team_name,\n",
    "                        'season': season\n",
    "                    }\n",
    "                    \n",
    "                    # Add all match data\n",
    "                    record.update(match)\n",
    "                    all_records.append(record)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(all_records)\n",
    "    \n",
    "    if len(df) > 0:\n",
    "        # Add full match report URL\n",
    "        df['full_match_report_url'] = 'https://fbref.com' + df['match_report_href']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Statistics Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_team_stats(soup: BeautifulSoup, match_id: str = None) -> Optional[Tuple[pd.DataFrame, str, str]]:\n",
    "    \"\"\"\n",
    "    Scrape main team statistics from match page in long format\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object of the match page\n",
    "        match_id: Match identifier (URL)\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (DataFrame with stats in long format, team1_name, team2_name) or None\n",
    "    \"\"\"\n",
    "    team_stats_div = soup.find('div', {'id': 'team_stats'})\n",
    "    \n",
    "    if not team_stats_div:\n",
    "        print(\"No team_stats div found\")\n",
    "        return None\n",
    "    \n",
    "    table = team_stats_div.find('table')\n",
    "    if not table:\n",
    "        print(\"No table found in team_stats div\")\n",
    "        return None\n",
    "    \n",
    "    # Extract team names from header - IMPROVED VERSION\n",
    "    header_row = table.find('tr')\n",
    "    team_cells = header_row.find_all('th')\n",
    "    \n",
    "    # Improved team name extraction - get full names, remove formation info\n",
    "    import re\n",
    "    team1_text = team_cells[0].get_text(strip=True)\n",
    "    team2_text = team_cells[1].get_text(strip=True)\n",
    "    \n",
    "    # Remove formation info (text in parentheses) if present\n",
    "    team1_name = re.sub(r'\\s*\\([^)]*\\)', '', team1_text).strip()\n",
    "    team2_name = re.sub(r'\\s*\\([^)]*\\)', '', team2_text).strip()\n",
    "    \n",
    "    # Fallback: if names are empty or too short, try alternative extraction\n",
    "    if not team1_name or not team2_name or len(team1_name) < 2 or len(team2_name) < 2:\n",
    "        print(\"Using fallback team name extraction...\")\n",
    "        fallback_names = extract_team_names_fallback(soup, match_id)\n",
    "        if fallback_names[0] and fallback_names[1]:\n",
    "            team1_name, team2_name = fallback_names\n",
    "    \n",
    "    print(f\"Teams found: '{team1_name}' vs '{team2_name}'\")\n",
    "    \n",
    "    # Parse stats in long format\n",
    "    stats_data = []\n",
    "    rows = table.find_all('tr')[1:]  # Skip header\n",
    "    \n",
    "    i = 0\n",
    "    while i < len(rows):\n",
    "        # Each stat has a header row followed by a data row\n",
    "        if i + 1 < len(rows):\n",
    "            header_row = rows[i]\n",
    "            data_row = rows[i + 1]\n",
    "            \n",
    "            # Get stat name\n",
    "            stat_name = header_row.get_text(strip=True)\n",
    "            \n",
    "            if stat_name and stat_name != \"Cards\":\n",
    "                # Get values for both teams\n",
    "                data_cells = data_row.find_all('td')\n",
    "                if len(data_cells) == 2:\n",
    "                    team1_value = data_cells[0].get_text(strip=True)\n",
    "                    team2_value = data_cells[1].get_text(strip=True)\n",
    "                    \n",
    "                    # Use improved extraction function\n",
    "                    team1_clean = extract_percentage_or_value(team1_value)\n",
    "                    team2_clean = extract_percentage_or_value(team2_value)\n",
    "                    \n",
    "                    # Add two rows: one for each team (long format)\n",
    "                    stats_data.append({\n",
    "                        'match_id': match_id,\n",
    "                        'team_name': team1_name,\n",
    "                        'stat_name': stat_name,\n",
    "                        'stat_value': team1_clean\n",
    "                    })\n",
    "                    stats_data.append({\n",
    "                        'match_id': match_id,\n",
    "                        'team_name': team2_name,\n",
    "                        'stat_name': stat_name,\n",
    "                        'stat_value': team2_clean\n",
    "                    })\n",
    "        \n",
    "        i += 2  # Skip to next stat (header + data)\n",
    "    \n",
    "    return pd.DataFrame(stats_data), team1_name, team2_name\n",
    "\n",
    "def scrape_team_stats_extra(soup: BeautifulSoup, team1_name: str, team2_name: str, match_id: str = None) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scrape extra team statistics from match page in long format\n",
    "    \n",
    "    Args:\n",
    "        soup: BeautifulSoup object of the match page\n",
    "        team1_name: Name of first team\n",
    "        team2_name: Name of second team\n",
    "        match_id: Match identifier (URL)\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with extra stats in long format or None\n",
    "    \"\"\"\n",
    "    team_stats_extra_div = soup.find('div', {'id': 'team_stats_extra'})\n",
    "    \n",
    "    if not team_stats_extra_div:\n",
    "        print(\"No team_stats_extra div found\")\n",
    "        return None\n",
    "    \n",
    "    stats_data = []\n",
    "    \n",
    "    # Find all stat containers\n",
    "    stat_containers = team_stats_extra_div.find_all('div', recursive=False)\n",
    "    \n",
    "    for container in stat_containers:\n",
    "        divs = container.find_all('div')\n",
    "        if len(divs) >= 3:\n",
    "            # Each row has: team1_value, stat_name, team2_value pattern\n",
    "            for i in range(0, len(divs), 3):\n",
    "                if i + 2 < len(divs):\n",
    "                    team1_value = divs[i].get_text(strip=True)\n",
    "                    stat_name = divs[i + 1].get_text(strip=True)\n",
    "                    team2_value = divs[i + 2].get_text(strip=True)\n",
    "                    \n",
    "                    # Skip headers and invalid data\n",
    "                    if team1_value.isdigit() and team2_value.isdigit():\n",
    "                        # Add two rows: one for each team (long format)\n",
    "                        stats_data.append({\n",
    "                            'match_id': match_id,\n",
    "                            'team_name': team1_name,\n",
    "                            'stat_name': stat_name,\n",
    "                            'stat_value': team1_value\n",
    "                        })\n",
    "                        stats_data.append({\n",
    "                            'match_id': match_id,\n",
    "                            'team_name': team2_name,\n",
    "                            'stat_name': stat_name,\n",
    "                            'stat_value': team2_value\n",
    "                        })\n",
    "    \n",
    "    return pd.DataFrame(stats_data) if stats_data else None\n",
    "\n",
    "def extract_team_names_fallback(soup, match_id=None):\n",
    "    \"\"\"\n",
    "    Fallback method to extract team names from other parts of the page\n",
    "    if header extraction fails\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Try to extract from page title\n",
    "    title = soup.find('title')\n",
    "    if title:\n",
    "        title_text = title.get_text()\n",
    "        # Title format is usually \"Team1 vs Team2, Date, Competition\"\n",
    "        match = re.search(r'(.+?)\\s+vs\\s+(.+?),', title_text)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "    \n",
    "    # Try to extract from H1 heading\n",
    "    h1 = soup.find('h1')\n",
    "    if h1:\n",
    "        h1_text = h1.get_text()\n",
    "        match = re.search(r'(.+?)\\s+vs\\.?\\s+(.+?)(?:\\s+\\(|$)', h1_text)\n",
    "        if match:\n",
    "            return match.group(1).strip(), match.group(2).strip()\n",
    "    \n",
    "    # Try to extract from URL if available\n",
    "    if match_id:\n",
    "        # URL format: /en/matches/id/Team1-Team2-Date-Competition\n",
    "        url_parts = match_id.split('/')\n",
    "        if len(url_parts) > 4:\n",
    "            match_info = url_parts[4]  # Team1-Team2-Date-Competition\n",
    "            parts = match_info.split('-')\n",
    "            if len(parts) >= 4:\n",
    "                # Usually first 1-3 parts are team1, next 1-3 are team2, then date\n",
    "                # This is a simplified approach - can be improved with team name mapping\n",
    "                mid = len(parts) // 2\n",
    "                team1_parts = parts[:mid]\n",
    "                team2_parts = parts[mid:mid*2]\n",
    "                \n",
    "                team1_name = ' '.join(team1_parts).replace('-', ' ')\n",
    "                team2_name = ' '.join(team2_parts).replace('-', ' ')\n",
    "                return team1_name, team2_name\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_match_stats(match_url: str) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Scrape all team stats (main + extra) from a single match URL in long format\n",
    "    \n",
    "    Args:\n",
    "        match_url: URL of the match page\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame with all team stats in long format or None\n",
    "        Format: match_id | team_name | stat_name | stat_value\n",
    "    \"\"\"\n",
    "    print(f\"Scraping: {match_url}\")\n",
    "    \n",
    "    # Fetch the page\n",
    "    soup = get_page(match_url)\n",
    "    if not soup:\n",
    "        print(f\"Failed to fetch page: {match_url}\")\n",
    "        return None\n",
    "    \n",
    "    # Scrape team stats\n",
    "    team_stats_result = scrape_team_stats(soup, match_url)\n",
    "    if team_stats_result is None:\n",
    "        print(\"Failed to scrape team stats\")\n",
    "        return None\n",
    "    \n",
    "    team_stats_df, team1_name, team2_name = team_stats_result\n",
    "    \n",
    "    # Scrape team stats extra\n",
    "    team_stats_extra_df = scrape_team_stats_extra(soup, team1_name, team2_name, match_url)\n",
    "    \n",
    "    # Concatenate the dataframes\n",
    "    dfs_to_concat = []\n",
    "    if team_stats_df is not None and len(team_stats_df) > 0:\n",
    "        dfs_to_concat.append(team_stats_df)\n",
    "        print(f\"Found {len(team_stats_df)} main stats rows\")\n",
    "    \n",
    "    if team_stats_extra_df is not None and len(team_stats_extra_df) > 0:\n",
    "        dfs_to_concat.append(team_stats_extra_df)\n",
    "        print(f\"Found {len(team_stats_extra_df)} extra stats rows\")\n",
    "    \n",
    "    if dfs_to_concat:\n",
    "        combined_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "        print(f\"Total stats collected: {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"No stats available\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_multiple_matches(match_urls: List[str], \n",
    "                          max_matches: Optional[int] = None,\n",
    "                          save_progress: bool = True,\n",
    "                          output_dir: str = '../../data/raw/match_stats/') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Scrape stats from multiple matches with progress saving\n",
    "    \n",
    "    Args:\n",
    "        match_urls: List of match URLs to scrape\n",
    "        max_matches: Maximum number of matches to process (None for all)\n",
    "        save_progress: Whether to save progress periodically\n",
    "        output_dir: Directory to save progress files\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame with all match stats\n",
    "    \"\"\"\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_stats = []\n",
    "    failed_urls = []\n",
    "    \n",
    "    # Limit matches if specified\n",
    "    urls_to_process = match_urls[:max_matches] if max_matches else match_urls\n",
    "    \n",
    "    print(f\"Processing {len(urls_to_process)} matches...\")\n",
    "    \n",
    "    for i, match_url in enumerate(urls_to_process, 1):\n",
    "        print(f\"\\n[{i}/{len(urls_to_process)}] Processing match...\")\n",
    "        \n",
    "        try:\n",
    "            stats_df = scrape_match_stats(match_url)\n",
    "            \n",
    "            if stats_df is not None:\n",
    "                all_stats.append(stats_df)\n",
    "                print(f\"✅ Successfully scraped {len(stats_df)} stats\")\n",
    "            else:\n",
    "                failed_urls.append(match_url)\n",
    "                print(f\"❌ Failed to scrape stats\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {match_url}: {e}\")\n",
    "            failed_urls.append(match_url)\n",
    "        \n",
    "        # Save progress every 10 matches\n",
    "        if save_progress and i % 10 == 0 and all_stats:\n",
    "            progress_df = pd.concat(all_stats, ignore_index=True)\n",
    "            progress_file = f\"{output_dir}progress_match_stats_{i}.json\"\n",
    "            progress_df.to_json(progress_file, orient='records', indent=2)\n",
    "            print(f\"💾 Progress saved: {progress_file}\")\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_stats:\n",
    "        final_df = pd.concat(all_stats, ignore_index=True)\n",
    "        print(f\"\\n✅ Successfully processed {len(all_stats)} matches\")\n",
    "        print(f\"❌ Failed to process {len(failed_urls)} matches\")\n",
    "        print(f\"📊 Total stats collected: {len(final_df)}\")\n",
    "        \n",
    "        # Save failed URLs for retry\n",
    "        if failed_urls:\n",
    "            failed_file = f\"{output_dir}failed_urls_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            with open(failed_file, 'w') as f:\n",
    "                json.dump(failed_urls, f, indent=2)\n",
    "            print(f\"📝 Failed URLs saved: {failed_file}\")\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"❌ No matches were successfully processed\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_match_stats(df: pd.DataFrame, filename: str, output_dir: str = '../../data/raw/match_stats/') -> None:\n",
    "    \"\"\"\n",
    "    Save match stats DataFrame to JSON file\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame to save\n",
    "        filename: Name of output file (without extension)\n",
    "        output_dir: Directory to save file\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    filepath = f\"{output_dir}{filename}.json\"\n",
    "    df.to_json(filepath, orient='records', indent=2)\n",
    "    print(f\"📁 Match stats saved: {filepath}\")\n",
    "\n",
    "def filter_fixtures_by_criteria(fixtures_df: pd.DataFrame, \n",
    "                               teams: Optional[List[str]] = None,\n",
    "                               seasons: Optional[List[str]] = None,\n",
    "                               competitions: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter fixtures DataFrame by various criteria\n",
    "    \n",
    "    Args:\n",
    "        fixtures_df: DataFrame with fixtures data\n",
    "        teams: List of team names to include\n",
    "        seasons: List of seasons to include\n",
    "        competitions: List of competitions to include\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame\n",
    "    \"\"\"\n",
    "    filtered_df = fixtures_df.copy()\n",
    "    \n",
    "    if teams:\n",
    "        filtered_df = filtered_df[filtered_df['team_name'].isin(teams)]\n",
    "    \n",
    "    if seasons:\n",
    "        filtered_df = filtered_df[filtered_df['season'].isin(seasons)]\n",
    "    \n",
    "    if competitions:\n",
    "        filtered_df = filtered_df[filtered_df['comp'].isin(competitions)]\n",
    "    \n",
    "    print(f\"Filtered to {len(filtered_df)} matches\")\n",
    "    return filtered_df\n",
    "\n",
    "def get_match_urls_from_fixtures(fixtures_df: pd.DataFrame) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extract match URLs from fixtures DataFrame\n",
    "    \n",
    "    Args:\n",
    "        fixtures_df: DataFrame with fixtures data\n",
    "    \n",
    "    Returns:\n",
    "        List of unique match URLs\n",
    "    \"\"\"\n",
    "    return fixtures_df['full_match_report_url'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Anti-Blocking Measures\n",
    "\n",
    "Enhanced scraping functions with better rate limiting, user-agent rotation, session management, and retry mechanisms to handle large-scale scraping (3000+ requests)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced scraper initialized with anti-blocking measures\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "import random\n",
    "import time\n",
    "from typing import List, Optional, Dict\n",
    "import json\n",
    "\n",
    "# Multiple User-Agents for rotation\n",
    "USER_AGENTS = [\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',\n",
    "    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',\n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "]\n",
    "\n",
    "class EnhancedScraper:\n",
    "    \"\"\"Enhanced scraper with anti-blocking measures\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 min_delay: float = 15.0,\n",
    "                 max_delay: float = 30.0,\n",
    "                 max_retries: int = 3,\n",
    "                 backoff_factor: float = 2.0,\n",
    "                 chunk_size: int = 50,\n",
    "                 chunk_break: float = 300.0):  # 5 minute break between chunks\n",
    "        \"\"\"\n",
    "        Initialize enhanced scraper\n",
    "        \n",
    "        Args:\n",
    "            min_delay: Minimum delay between requests (seconds)\n",
    "            max_delay: Maximum delay between requests (seconds) \n",
    "            max_retries: Maximum retry attempts for failed requests\n",
    "            backoff_factor: Exponential backoff multiplier\n",
    "            chunk_size: Number of requests per chunk before long break\n",
    "            chunk_break: Break time between chunks (seconds)\n",
    "        \"\"\"\n",
    "        self.min_delay = min_delay\n",
    "        self.max_delay = max_delay\n",
    "        self.max_retries = max_retries\n",
    "        self.backoff_factor = backoff_factor\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_break = chunk_break\n",
    "        \n",
    "        # Request counter for chunking\n",
    "        self.request_count = 0\n",
    "        \n",
    "        # Create session with retry strategy\n",
    "        self.session = self._create_session()\n",
    "        \n",
    "    def _create_session(self) -> requests.Session:\n",
    "        \"\"\"Create requests session with retry strategy\"\"\"\n",
    "        session = requests.Session()\n",
    "        \n",
    "        # Retry strategy\n",
    "        retry_strategy = Retry(\n",
    "            total=self.max_retries,\n",
    "            status_forcelist=[429, 500, 502, 503, 504],\n",
    "            backoff_factor=self.backoff_factor,\n",
    "            respect_retry_after_header=True\n",
    "        )\n",
    "        \n",
    "        adapter = HTTPAdapter(max_retries=retry_strategy)\n",
    "        session.mount(\"http://\", adapter)\n",
    "        session.mount(\"https://\", adapter)\n",
    "        \n",
    "        return session\n",
    "    \n",
    "    def _get_random_headers(self) -> Dict[str, str]:\n",
    "        \"\"\"Get random headers for request\"\"\"\n",
    "        return {\n",
    "            'User-Agent': random.choice(USER_AGENTS),\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "            'Accept-Language': 'en-US,en;q=0.5',\n",
    "            'Accept-Encoding': 'gzip, deflate',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "        }\n",
    "    \n",
    "    def _wait_between_requests(self):\n",
    "        \"\"\"Handle delays and chunking\"\"\"\n",
    "        # Random delay between requests\n",
    "        delay = random.uniform(self.min_delay, self.max_delay)\n",
    "        print(f\"⏱️  Waiting {delay:.1f} seconds...\")\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        # Increment request counter\n",
    "        self.request_count += 1\n",
    "        \n",
    "        # Check if we need a chunk break\n",
    "        if self.request_count % self.chunk_size == 0:\n",
    "            print(f\"\\n🛑 Chunk break after {self.request_count} requests\")\n",
    "            print(f\"⏰ Waiting {self.chunk_break/60:.1f} minutes before continuing...\")\n",
    "            time.sleep(self.chunk_break)\n",
    "            print(\"🚀 Resuming scraping...\\n\")\n",
    "    \n",
    "    def get_page_enhanced(self, url: str) -> Optional[BeautifulSoup]:\n",
    "        \"\"\"\n",
    "        Enhanced page fetching with anti-blocking measures\n",
    "        \n",
    "        Args:\n",
    "            url: URL to fetch\n",
    "            \n",
    "        Returns:\n",
    "            BeautifulSoup object or None if failed\n",
    "        \"\"\"\n",
    "        self._wait_between_requests()\n",
    "        \n",
    "        headers = self._get_random_headers()\n",
    "        \n",
    "        try:\n",
    "            response = self.session.get(url, headers=headers, timeout=30)\n",
    "            \n",
    "            # Check for rate limiting\n",
    "            if response.status_code == 429:\n",
    "                print(f\"⚠️  Rate limited. Waiting longer...\")\n",
    "                time.sleep(60)  # Wait 1 minute for rate limit\n",
    "                return None\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            return BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "        except requests.RequestException as e:\n",
    "            print(f\"❌ Error fetching {url}: {e}\")\n",
    "            \n",
    "            # Exponential backoff for retries\n",
    "            if \"429\" in str(e) or \"rate\" in str(e).lower():\n",
    "                wait_time = 60 * (2 ** (self.request_count % 3))  # Exponential backoff\n",
    "                print(f\"⏰ Rate limited. Waiting {wait_time/60:.1f} minutes...\")\n",
    "                time.sleep(wait_time)\n",
    "            \n",
    "            return None\n",
    "    \n",
    "    def scrape_match_stats_enhanced(self, match_url: str) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Enhanced version of scrape_match_stats with anti-blocking measures\n",
    "        \"\"\"\n",
    "        print(f\"🔍 Scraping: {match_url}\")\n",
    "        \n",
    "        # Fetch the page with enhanced measures\n",
    "        soup = self.get_page_enhanced(match_url)\n",
    "        if not soup:\n",
    "            print(f\"❌ Failed to fetch page: {match_url}\")\n",
    "            return None\n",
    "        \n",
    "        # Use existing scraping logic\n",
    "        team_stats_result = scrape_team_stats(soup, match_url)\n",
    "        if team_stats_result is None:\n",
    "            print(\"❌ Failed to scrape team stats\")\n",
    "            return None\n",
    "        \n",
    "        team_stats_df, team1_name, team2_name = team_stats_result\n",
    "        \n",
    "        # Scrape team stats extra\n",
    "        team_stats_extra_df = scrape_team_stats_extra(soup, team1_name, team2_name, match_url)\n",
    "        \n",
    "        # Combine results\n",
    "        dfs_to_concat = []\n",
    "        if team_stats_df is not None and len(team_stats_df) > 0:\n",
    "            dfs_to_concat.append(team_stats_df)\n",
    "            print(f\"✅ Found {len(team_stats_df)} main stats rows\")\n",
    "        \n",
    "        if team_stats_extra_df is not None and len(team_stats_extra_df) > 0:\n",
    "            dfs_to_concat.append(team_stats_extra_df)\n",
    "            print(f\"✅ Found {len(team_stats_extra_df)} extra stats rows\")\n",
    "        \n",
    "        if dfs_to_concat:\n",
    "            combined_df = pd.concat(dfs_to_concat, ignore_index=True)\n",
    "            print(f\"📊 Total stats collected: {len(combined_df)} rows\")\n",
    "            return combined_df\n",
    "        else:\n",
    "            print(\"⚠️  No stats available\")\n",
    "            return None\n",
    "\n",
    "# Initialize enhanced scraper\n",
    "enhanced_scraper = EnhancedScraper(\n",
    "    min_delay=15.0,      # 15-30 second delays\n",
    "    max_delay=30.0,\n",
    "    chunk_size=50,       # Break every 50 requests\n",
    "    chunk_break=300.0    # 5 minute breaks\n",
    ")\n",
    "\n",
    "print(\"✅ Enhanced scraper initialized with anti-blocking measures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to test enhanced scraper!\n"
     ]
    }
   ],
   "source": [
    "def scrape_multiple_matches_enhanced(match_urls: List[str], \n",
    "                                   max_matches: Optional[int] = None,\n",
    "                                   save_progress: bool = True,\n",
    "                                   output_dir: str = '../../data/dev/raw/match_stats_v2/',\n",
    "                                   scraper: EnhancedScraper = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Enhanced batch scraping with anti-blocking measures\n",
    "    \n",
    "    Args:\n",
    "        match_urls: List of match URLs to scrape\n",
    "        max_matches: Maximum number of matches to process\n",
    "        save_progress: Whether to save progress periodically\n",
    "        output_dir: Directory to save progress files\n",
    "        scraper: EnhancedScraper instance to use\n",
    "    \n",
    "    Returns:\n",
    "        Combined DataFrame with all match stats\n",
    "    \"\"\"\n",
    "    if scraper is None:\n",
    "        scraper = enhanced_scraper\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    all_stats = []\n",
    "    failed_urls = []\n",
    "    \n",
    "    # Limit matches if specified\n",
    "    urls_to_process = match_urls[:max_matches] if max_matches else match_urls\n",
    "    \n",
    "    print(f\"🚀 Starting enhanced scraping for {len(urls_to_process)} matches\")\n",
    "    print(f\"⚙️  Settings: {scraper.min_delay}-{scraper.max_delay}s delays, {scraper.chunk_size} requests per chunk\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i, match_url in enumerate(urls_to_process, 1):\n",
    "        print(f\"\\n[{i}/{len(urls_to_process)}] Processing match...\")\n",
    "        \n",
    "        try:\n",
    "            stats_df = scraper.scrape_match_stats_enhanced(match_url)\n",
    "            \n",
    "            if stats_df is not None:\n",
    "                all_stats.append(stats_df)\n",
    "                print(f\"✅ Successfully scraped {len(stats_df)} stats\")\n",
    "            else:\n",
    "                failed_urls.append(match_url)\n",
    "                print(f\"❌ Failed to scrape stats\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error processing {match_url}: {e}\")\n",
    "            failed_urls.append(match_url)\n",
    "        \n",
    "        # Save progress every 20 matches (more frequent saves)\n",
    "        if save_progress and i % 20 == 0 and all_stats:\n",
    "            progress_df = pd.concat(all_stats, ignore_index=True)\n",
    "            progress_file = f\"{output_dir}progress_enhanced_{i}.json\"\n",
    "            progress_df.to_json(progress_file, orient='records', indent=2)\n",
    "            \n",
    "            elapsed = time.time() - start_time\n",
    "            rate = i / elapsed * 3600  # matches per hour\n",
    "            print(f\"💾 Progress saved: {progress_file}\")\n",
    "            print(f\"📈 Rate: {rate:.1f} matches/hour\")\n",
    "    \n",
    "    # Final results\n",
    "    if all_stats:\n",
    "        final_df = pd.concat(all_stats, ignore_index=True)\n",
    "        \n",
    "        elapsed = time.time() - start_time\n",
    "        total_time = elapsed / 3600  # hours\n",
    "        \n",
    "        print(f\"\\n🎉 SCRAPING COMPLETE!\")\n",
    "        print(f\"✅ Successfully processed: {len(all_stats)} matches\")\n",
    "        print(f\"❌ Failed to process: {len(failed_urls)} matches\")\n",
    "        print(f\"📊 Total stats collected: {len(final_df)} rows\")\n",
    "        print(f\"⏰ Total time: {total_time:.2f} hours\")\n",
    "        print(f\"📈 Average rate: {len(all_stats)/total_time:.1f} matches/hour\")\n",
    "        \n",
    "        # Save failed URLs\n",
    "        if failed_urls:\n",
    "            failed_file = f\"{output_dir}failed_urls_enhanced_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "            with open(failed_file, 'w') as f:\n",
    "                json.dump(failed_urls, f, indent=2)\n",
    "            print(f\"📝 Failed URLs saved: {failed_file}\")\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"❌ No matches were successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Test the enhanced scraper with a small sample\n",
    "print(\"Ready to test enhanced scraper!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage Example - Enhanced Scraper\n",
    "\n",
    "Example of how to use the enhanced scraper with anti-blocking measures for large-scale scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3272 unique match URLs\n",
      "\n",
      "Fixtures preview:\n",
      "  team_name     season            comp  \\\n",
      "0   Arsenal  2019-2020  Premier League   \n",
      "1   Arsenal  2019-2020  Premier League   \n",
      "2   Arsenal  2019-2020  Premier League   \n",
      "3   Arsenal  2019-2020  Premier League   \n",
      "4   Arsenal  2019-2020  Premier League   \n",
      "\n",
      "                                   match_report_href  \n",
      "0  /en/matches/1405a610/Newcastle-United-Arsenal-...  \n",
      "1  /en/matches/ff7eda21/Arsenal-Burnley-August-17...  \n",
      "2  /en/matches/102b241e/Liverpool-Arsenal-August-...  \n",
      "3  /en/matches/0b6b8aaf/North-London-Derby-Arsena...  \n",
      "4  /en/matches/8257eda8/Watford-Arsenal-September...  \n"
     ]
    }
   ],
   "source": [
    "# Load fixtures data (if not already loaded)\n",
    "if 'fixtures_df' not in locals():\n",
    "    print(\"Loading fixtures data...\")\n",
    "    fixtures_df = load_fixtures_from_json('../../../data/dev/raw/all_competitions_fixtures_2019_2025.json')\n",
    "    print(f\"Loaded {len(fixtures_df)} total fixtures\")\n",
    "\n",
    "# Get all match URLs\n",
    "match_urls = get_match_urls_from_fixtures(fixtures_df)\n",
    "print(f\"Found {len(match_urls)} unique match URLs\")\n",
    "\n",
    "# Preview the data\n",
    "print(\"\\nFixtures preview:\")\n",
    "print(fixtures_df[['team_name', 'season', 'comp', 'match_report_href']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STEP 1: Test with a small sample first (RECOMMENDED)\n",
    "# print(\"🧪 TESTING ENHANCED SCRAPER WITH SMALL SAMPLE\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# # Test with just 3 matches first\n",
    "# test_urls = match_urls[:25]\n",
    "# print(f\"Testing with {len(test_urls)} matches...\")\n",
    "\n",
    "# # Create a test scraper with faster settings for testing\n",
    "# test_scraper = EnhancedScraper(\n",
    "#     min_delay=4.0,       # Faster for testing\n",
    "#     max_delay=10.0,\n",
    "#     chunk_size=30,       # Smaller chunks for testing\n",
    "#     chunk_break=10     # Shorter breaks for testing\n",
    "# )\n",
    "\n",
    "# # Run test\n",
    "# test_stats_df = scrape_multiple_matches_enhanced(\n",
    "#     test_urls, \n",
    "#     max_matches=35,\n",
    "#     scraper=test_scraper,\n",
    "#     output_dir='../../data/dev/raw/match_stats/test/'\n",
    "# )\n",
    "\n",
    "# if not test_stats_df.empty:\n",
    "#     print(f\"\\n✅ TEST SUCCESSFUL!\")\n",
    "#     print(f\"Collected {len(test_stats_df)} stats rows\")\n",
    "#     print(f\"Sample data:\")\n",
    "#     print(test_stats_df.head())\n",
    "# else:\n",
    "#     print(\"\\n❌ TEST FAILED - Check the output above for errors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # STEP 2: Production scraping with enhanced measures\n",
    "# print(\"🚀 PRODUCTION SCRAPING WITH ENHANCED MEASURES\")\n",
    "# print(\"=\" * 50)\n",
    "\n",
    "# Configure production scraper (uncomment to run)\n",
    "production_scraper = EnhancedScraper(\n",
    "    min_delay=3.0,      # 15-30 second delays for safety\n",
    "    max_delay=12.0,\n",
    "    chunk_size=250,       # Break every 50 requests  \n",
    "    chunk_break=500.0    # 5 minute breaks between chunks\n",
    ")\n",
    "\n",
    "# print(f\"📊 Total matches to scrape: {len(match_urls)}\")\n",
    "# print(f\"⏰ Estimated time: ~{len(match_urls) * 22.5 / 3600:.1f} hours\")\n",
    "# print(f\"⚙️  Settings: {production_scraper.min_delay}-{production_scraper.max_delay}s delays\")\n",
    "# print(f\"🛑 Breaks: {production_scraper.chunk_break/60:.0f} min every {production_scraper.chunk_size} requests\")\n",
    "\n",
    "# # UNCOMMENT THE LINES BELOW TO START PRODUCTION SCRAPING\n",
    "# # WARNING: This will take ~27-30 hours to complete all 3272 matches\n",
    "\n",
    "# enhanced_stats_df = scrape_multiple_matches_enhanced(\n",
    "#     match_urls,\n",
    "#     max_matches=None,  # Process all matches\n",
    "#     scraper=production_scraper,\n",
    "#     output_dir='../../data/prod/raw/match_stats/complete_names/'\n",
    "# )\n",
    "\n",
    "# # Save final results\n",
    "# if not enhanced_stats_df.empty:\n",
    "#     save_match_stats(\n",
    "#         enhanced_stats_df, \n",
    "#         'all_competitions_enhanced_match_stats_2019_2025',\n",
    "#         output_dir='../../data/prod/raw/match_stats/complete_names/not_scraped/'\n",
    "#     )\n",
    "#     print(f\"\\n🎉 COMPLETE! Final dataset: {len(enhanced_stats_df)} rows\")\n",
    "\n",
    "# print(\"\\n⚠️  PRODUCTION SCRAPING IS COMMENTED OUT\")\n",
    "# print(\"Uncomment the lines above to start the full scraping process\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 RESUME FROM PARTIAL RUN (if needed)\n",
      "========================================\n",
      "Found 3231 completed matches\n",
      "Remaining: 41 matches\n",
      "🚀 Starting enhanced scraping for 41 matches\n",
      "⚙️  Settings: 3.0-12.0s delays, 250 requests per chunk\n",
      "\n",
      "[1/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/8323ca40/Arsenal-Manchester-United-January-12-2025-FA-Cup\n",
      "⏱️  Waiting 3.7 seconds...\n",
      "Teams found: 'Arsenal' vs 'Manchester Utd'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[2/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/f36a1b2f/Hibernian-Aston-Villa-August-23-2023-Europa-Conference-League\n",
      "⏱️  Waiting 4.1 seconds...\n",
      "Teams found: 'Hibernian' vs 'Aston Villa'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[3/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/5cc4d456/Aston-Villa-Hibernian-August-31-2023-Europa-Conference-League\n",
      "⏱️  Waiting 11.2 seconds...\n",
      "Teams found: 'Aston Villa' vs 'Hibernian'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[4/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/027fb10f/Aston-Villa-West-Ham-United-January-10-2025-FA-Cup\n",
      "⏱️  Waiting 10.7 seconds...\n",
      "Teams found: 'Aston Villa' vs 'West Ham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[5/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/955c44a4/Bournemouth-West-Bromwich-Albion-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 5.2 seconds...\n",
      "Teams found: 'Bournemouth' vs 'West Brom'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[6/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/eebe09b9/Tottenham-Hotspur-Brighton-and-Hove-Albion-December-26-2019-Premier-League\n",
      "⏱️  Waiting 7.1 seconds...\n",
      "Teams found: 'Tottenham' vs 'Brighton'\n",
      "✅ Found 8 main stats rows\n",
      "✅ Found 24 extra stats rows\n",
      "📊 Total stats collected: 32 rows\n",
      "✅ Successfully scraped 32 stats\n",
      "\n",
      "[7/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/1d0fba4b/Norwich-City-Brighton-and-Hove-Albion-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 3.2 seconds...\n",
      "Teams found: 'Norwich City' vs 'Brighton'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[8/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/3b42d2cf/Chelsea-Servette-FC-August-22-2024-Conference-League\n",
      "⏱️  Waiting 10.0 seconds...\n",
      "Teams found: 'Chelsea' vs 'Servette FC'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[9/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/229b3cc6/Servette-FC-Chelsea-August-29-2024-Conference-League\n",
      "⏱️  Waiting 3.5 seconds...\n",
      "Teams found: 'Servette FC' vs 'Chelsea'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[10/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/202055c1/Chelsea-Morecambe-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 6.1 seconds...\n",
      "Teams found: 'Chelsea' vs 'Morecambe'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[11/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/01cc8150/Crystal-Palace-Stockport-January-12-2025-FA-Cup\n",
      "⏱️  Waiting 9.2 seconds...\n",
      "Teams found: 'Crystal Palace' vs 'Stockport'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[12/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/fde1af2b/Everton-Peterborough-United-January-9-2025-FA-Cup\n",
      "⏱️  Waiting 3.0 seconds...\n",
      "Teams found: 'Everton' vs 'P'borough Utd'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[13/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/6aa00240/Leicester-City-Queens-Park-Rangers-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 3.6 seconds...\n",
      "Teams found: 'Leicester City' vs 'QPR'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[14/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/71f78ca3/Liverpool-Accrington-Stanley-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 8.2 seconds...\n",
      "Teams found: 'Liverpool' vs 'Acc'ton Stanley'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[15/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/5ce15b58/Leeds-United-Manchester-City-October-3-2020-Premier-League\n",
      "⏱️  Waiting 3.7 seconds...\n",
      "Teams found: 'Leeds United' vs 'Manchester City'\n",
      "✅ Found 8 main stats rows\n",
      "✅ Found 24 extra stats rows\n",
      "📊 Total stats collected: 32 rows\n",
      "✅ Successfully scraped 32 stats\n",
      "\n",
      "[16/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/27966ef3/Manchester-City-Porto-October-21-2020-Champions-League\n",
      "⏱️  Waiting 9.5 seconds...\n",
      "Teams found: 'Manchester City' vs 'Porto'\n",
      "✅ Found 8 main stats rows\n",
      "✅ Found 24 extra stats rows\n",
      "📊 Total stats collected: 32 rows\n",
      "✅ Successfully scraped 32 stats\n",
      "\n",
      "[17/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/2b0c0eca/West-Ham-United-Manchester-City-October-24-2020-Premier-League\n",
      "⏱️  Waiting 9.1 seconds...\n",
      "Teams found: 'West Ham' vs 'Manchester City'\n",
      "✅ Found 8 main stats rows\n",
      "✅ Found 24 extra stats rows\n",
      "📊 Total stats collected: 32 rows\n",
      "✅ Successfully scraped 32 stats\n",
      "\n",
      "[18/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/08a2f03b/Marseille-Manchester-City-October-27-2020-Champions-League\n",
      "⏱️  Waiting 11.8 seconds...\n",
      "Teams found: 'Marseille' vs 'Manchester City'\n",
      "✅ Found 8 main stats rows\n",
      "✅ Found 24 extra stats rows\n",
      "📊 Total stats collected: 32 rows\n",
      "✅ Successfully scraped 32 stats\n",
      "\n",
      "[19/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/b6085db1/Manchester-City-Salford-City-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 7.0 seconds...\n",
      "Teams found: 'Manchester City' vs 'Salford City'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[20/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/52b06972/Newcastle-United-Bromley-January-12-2025-FA-Cup\n",
      "⏱️  Waiting 7.4 seconds...\n",
      "Teams found: 'Newcastle Utd' vs 'Bromley'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "💾 Progress saved: ../../data/prod/raw/match_stats/complete_names/rescraped_matches/progress_enhanced_20.json\n",
      "📈 Rate: 483.6 matches/hour\n",
      "\n",
      "[21/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/fb9f6709/Southampton-Swansea-City-January-12-2025-FA-Cup\n",
      "⏱️  Waiting 8.0 seconds...\n",
      "Teams found: 'Southampton' vs 'Swansea City'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[22/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/05180b72/Lokomotiv-Plovdiv-Tottenham-Hotspur-September-17-2020-Europa-League\n",
      "⏱️  Waiting 9.8 seconds...\n",
      "Teams found: 'Loko Plovdiv' vs 'Tottenham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[23/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/23ede0b1/Leyton-Orient-Tottenham-Hotspur-September-22-2020-EFL-Cup\n",
      "⏱️  Waiting 10.6 seconds...\n",
      "Teams found: 'Leyton Orient' vs 'Tottenham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[24/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/4f31c833/Shkendija-79-Tottenham-Hotspur-September-24-2020-Europa-League\n",
      "⏱️  Waiting 3.1 seconds...\n",
      "Teams found: 'Shkëndija 79' vs 'Tottenham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[25/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/e9098b8d/Pacos-de-Ferreira-Tottenham-Hotspur-August-19-2021-Europa-Conference-League\n",
      "⏱️  Waiting 8.4 seconds...\n",
      "Teams found: 'Paços de Ferreira' vs 'Tottenham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[26/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/c2e330d2/Tottenham-Hotspur-Pacos-de-Ferreira-August-26-2021-Europa-Conference-League\n",
      "⏱️  Waiting 9.6 seconds...\n",
      "Teams found: 'Tottenham' vs 'Paços de Ferreira'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[27/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/d69a0a11/Tottenham-Hotspur-Rennes-December-9-2021-Europa-Conference-League\n",
      "⏱️  Waiting 10.7 seconds...\n",
      "Teams found: 'Tottenham' vs 'Rennes'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[28/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/168af0ed/Tamworth-Tottenham-Hotspur-January-12-2025-FA-Cup\n",
      "⏱️  Waiting 6.9 seconds...\n",
      "Teams found: 'Tamworth' vs 'Tottenham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[29/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/3d017f98/West-Ham-United-Viborg-August-18-2022-Europa-Conference-League\n",
      "⏱️  Waiting 5.0 seconds...\n",
      "Teams found: 'West Ham' vs 'Viborg'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[30/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/186dcc75/Viborg-West-Ham-United-August-25-2022-Europa-Conference-League\n",
      "⏱️  Waiting 8.8 seconds...\n",
      "Teams found: 'Viborg' vs 'West Ham'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[31/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/4351690b/Wolverhampton-Wanderers-Crusaders-July-25-2019-Europa-League\n",
      "⏱️  Waiting 9.1 seconds...\n",
      "Teams found: 'Wolves' vs 'Crusaders'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[32/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/6cdcf068/Crusaders-Wolverhampton-Wanderers-August-1-2019-Europa-League\n",
      "⏱️  Waiting 3.3 seconds...\n",
      "Teams found: 'Crusaders' vs 'Wolves'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[33/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/7f00c0f3/Pyunik-Wolverhampton-Wanderers-August-8-2019-Europa-League\n",
      "⏱️  Waiting 5.0 seconds...\n",
      "Teams found: 'Pyunik' vs 'Wolves'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[34/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/d8224a3b/Wolverhampton-Wanderers-Pyunik-August-15-2019-Europa-League\n",
      "⏱️  Waiting 3.6 seconds...\n",
      "Teams found: 'Wolves' vs 'Pyunik'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[35/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/ce4ab52c/Brentford-Wolverhampton-Wanderers-January-5-2024-FA-Cup\n",
      "⏱️  Waiting 8.3 seconds...\n",
      "Teams found: 'Brentford' vs 'Wolves'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[36/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/b07a4363/Wolverhampton-Wanderers-Brentford-January-16-2024-FA-Cup\n",
      "⏱️  Waiting 10.5 seconds...\n",
      "Teams found: 'Wolves' vs 'Brentford'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[37/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/99e0a2ef/Bristol-City-Wolverhampton-Wanderers-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 11.5 seconds...\n",
      "Teams found: 'Bristol City' vs 'Wolves'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[38/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/0485a286/Fulham-Watford-January-9-2025-FA-Cup\n",
      "⏱️  Waiting 7.9 seconds...\n",
      "Teams found: 'Fulham' vs 'Watford'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[39/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/b720371c/Brentford-Plymouth-Argyle-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 11.7 seconds...\n",
      "Teams found: 'Brentford' vs 'Plymouth Argyle'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "[40/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/badf2ff8/Nottingham-Forest-Luton-Town-January-11-2025-FA-Cup\n",
      "⏱️  Waiting 10.3 seconds...\n",
      "Teams found: 'Nott'ham Forest' vs 'Luton Town'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "💾 Progress saved: ../../data/prod/raw/match_stats/complete_names/rescraped_matches/progress_enhanced_40.json\n",
      "📈 Rate: 452.0 matches/hour\n",
      "\n",
      "[41/41] Processing match...\n",
      "🔍 Scraping: https://fbref.com/en/matches/f774f3d6/Ipswich-Town-Bristol-Rovers-January-12-2025-FA-Cup\n",
      "⏱️  Waiting 7.0 seconds...\n",
      "Teams found: 'Ipswich Town' vs 'Bristol Rovers'\n",
      "No team_stats_extra div found\n",
      "⚠️  No stats available\n",
      "❌ Failed to scrape stats\n",
      "\n",
      "🎉 SCRAPING COMPLETE!\n",
      "✅ Successfully processed: 5 matches\n",
      "❌ Failed to process: 36 matches\n",
      "📊 Total stats collected: 160 rows\n",
      "⏰ Total time: 0.09 hours\n",
      "📈 Average rate: 55.2 matches/hour\n",
      "📝 Failed URLs saved: ../../data/prod/raw/match_stats/complete_names/rescraped_matches/failed_urls_enhanced_20250802_222734.json\n",
      "Resume functionality available - uncomment code above if needed\n"
     ]
    }
   ],
   "source": [
    "# STEP 3: Optional - Resume from failed/partial run\n",
    "print(\"🔄 RESUME FROM PARTIAL RUN (if needed)\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# If you need to resume scraping from where you left off:\n",
    "\n",
    "# 1. Load progress file (example)\n",
    "progress_file = '../../data/prod/raw/match_stats/complete_names/all_competitions_enhanced_match_stats_2019_2025.json'\n",
    "if os.path.exists(progress_file):\n",
    "    completed_df = pd.read_json(progress_file)\n",
    "    completed_urls = completed_df['match_id'].unique()\n",
    "    remaining_urls = [url for url in match_urls if url not in completed_urls]\n",
    "    print(f\"Found {len(completed_urls)} completed matches\")\n",
    "    print(f\"Remaining: {len(remaining_urls)} matches\")\n",
    "else:\n",
    "    remaining_urls = match_urls\n",
    "    print(\"No progress file found, starting from beginning\")\n",
    "\n",
    "# 2. Continue with remaining URLs\n",
    "enhanced_stats_df = scrape_multiple_matches_enhanced(\n",
    "    remaining_urls,\n",
    "    scraper=production_scraper,\n",
    "    output_dir='../../data/prod/raw/match_stats/complete_names/rescraped_matches/'\n",
    ")\n",
    "\n",
    "print(\"Resume functionality available - uncomment code above if needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>match_id</th>\n",
       "      <th>team_name_x</th>\n",
       "      <th>stat_name</th>\n",
       "      <th>stat_value</th>\n",
       "      <th>season</th>\n",
       "      <th>date</th>\n",
       "      <th>start_time</th>\n",
       "      <th>comp</th>\n",
       "      <th>round</th>\n",
       "      <th>dayofweek</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://fbref.com/en/matches/1405a610/Newcastl...</td>\n",
       "      <td>Newcastle Utd</td>\n",
       "      <td>Possession</td>\n",
       "      <td>38%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>14:00</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>Matchweek 1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://fbref.com/en/matches/1405a610/Newcastl...</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Possession</td>\n",
       "      <td>62%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>14:00</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>Matchweek 1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://fbref.com/en/matches/1405a610/Newcastl...</td>\n",
       "      <td>Newcastle Utd</td>\n",
       "      <td>Passing Accuracy</td>\n",
       "      <td>75%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>14:00</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>Matchweek 1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://fbref.com/en/matches/1405a610/Newcastl...</td>\n",
       "      <td>Arsenal</td>\n",
       "      <td>Passing Accuracy</td>\n",
       "      <td>84%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>14:00</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>Matchweek 1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://fbref.com/en/matches/1405a610/Newcastl...</td>\n",
       "      <td>Newcastle Utd</td>\n",
       "      <td>Shots on Target</td>\n",
       "      <td>22%</td>\n",
       "      <td>2019-2020</td>\n",
       "      <td>2019-08-11</td>\n",
       "      <td>14:00</td>\n",
       "      <td>Premier League</td>\n",
       "      <td>Matchweek 1</td>\n",
       "      <td>Sun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174881</th>\n",
       "      <td>https://fbref.com/en/matches/be42686a/Coventry...</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>Crosses</td>\n",
       "      <td>9</td>\n",
       "      <td>2024-2025</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>15:00</td>\n",
       "      <td>FA Cup</td>\n",
       "      <td>Fourth round proper</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174882</th>\n",
       "      <td>https://fbref.com/en/matches/be42686a/Coventry...</td>\n",
       "      <td>Coventry City</td>\n",
       "      <td>Interceptions</td>\n",
       "      <td>13</td>\n",
       "      <td>2024-2025</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>15:00</td>\n",
       "      <td>FA Cup</td>\n",
       "      <td>Fourth round proper</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174883</th>\n",
       "      <td>https://fbref.com/en/matches/be42686a/Coventry...</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>Interceptions</td>\n",
       "      <td>14</td>\n",
       "      <td>2024-2025</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>15:00</td>\n",
       "      <td>FA Cup</td>\n",
       "      <td>Fourth round proper</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174884</th>\n",
       "      <td>https://fbref.com/en/matches/be42686a/Coventry...</td>\n",
       "      <td>Coventry City</td>\n",
       "      <td>Offsides</td>\n",
       "      <td>3</td>\n",
       "      <td>2024-2025</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>15:00</td>\n",
       "      <td>FA Cup</td>\n",
       "      <td>Fourth round proper</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174885</th>\n",
       "      <td>https://fbref.com/en/matches/be42686a/Coventry...</td>\n",
       "      <td>Ipswich Town</td>\n",
       "      <td>Offsides</td>\n",
       "      <td>4</td>\n",
       "      <td>2024-2025</td>\n",
       "      <td>2025-02-08</td>\n",
       "      <td>15:00</td>\n",
       "      <td>FA Cup</td>\n",
       "      <td>Fourth round proper</td>\n",
       "      <td>Sat</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98004 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 match_id    team_name_x  \\\n",
       "0       https://fbref.com/en/matches/1405a610/Newcastl...  Newcastle Utd   \n",
       "2       https://fbref.com/en/matches/1405a610/Newcastl...        Arsenal   \n",
       "4       https://fbref.com/en/matches/1405a610/Newcastl...  Newcastle Utd   \n",
       "6       https://fbref.com/en/matches/1405a610/Newcastl...        Arsenal   \n",
       "8       https://fbref.com/en/matches/1405a610/Newcastl...  Newcastle Utd   \n",
       "...                                                   ...            ...   \n",
       "174881  https://fbref.com/en/matches/be42686a/Coventry...   Ipswich Town   \n",
       "174882  https://fbref.com/en/matches/be42686a/Coventry...  Coventry City   \n",
       "174883  https://fbref.com/en/matches/be42686a/Coventry...   Ipswich Town   \n",
       "174884  https://fbref.com/en/matches/be42686a/Coventry...  Coventry City   \n",
       "174885  https://fbref.com/en/matches/be42686a/Coventry...   Ipswich Town   \n",
       "\n",
       "               stat_name stat_value     season        date start_time  \\\n",
       "0             Possession        38%  2019-2020  2019-08-11      14:00   \n",
       "2             Possession        62%  2019-2020  2019-08-11      14:00   \n",
       "4       Passing Accuracy        75%  2019-2020  2019-08-11      14:00   \n",
       "6       Passing Accuracy        84%  2019-2020  2019-08-11      14:00   \n",
       "8        Shots on Target        22%  2019-2020  2019-08-11      14:00   \n",
       "...                  ...        ...        ...         ...        ...   \n",
       "174881           Crosses          9  2024-2025  2025-02-08      15:00   \n",
       "174882     Interceptions         13  2024-2025  2025-02-08      15:00   \n",
       "174883     Interceptions         14  2024-2025  2025-02-08      15:00   \n",
       "174884          Offsides          3  2024-2025  2025-02-08      15:00   \n",
       "174885          Offsides          4  2024-2025  2025-02-08      15:00   \n",
       "\n",
       "                  comp                round dayofweek  \n",
       "0       Premier League          Matchweek 1       Sun  \n",
       "2       Premier League          Matchweek 1       Sun  \n",
       "4       Premier League          Matchweek 1       Sun  \n",
       "6       Premier League          Matchweek 1       Sun  \n",
       "8       Premier League          Matchweek 1       Sun  \n",
       "...                ...                  ...       ...  \n",
       "174881          FA Cup  Fourth round proper       Sat  \n",
       "174882          FA Cup  Fourth round proper       Sat  \n",
       "174883          FA Cup  Fourth round proper       Sat  \n",
       "174884          FA Cup  Fourth round proper       Sat  \n",
       "174885          FA Cup  Fourth round proper       Sat  \n",
       "\n",
       "[98004 rows x 10 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_select = [\n",
    "    'match_id', \n",
    "    'team_name_x', \n",
    "    'stat_name', \n",
    "    'stat_value',\n",
    "    'season', \n",
    "    'date', \n",
    "    'start_time',\n",
    "    'comp',\n",
    "    'round',\n",
    "    'dayofweek'\n",
    "]\n",
    "\n",
    "\n",
    "completed_df.merge(\n",
    "    fixtures_df,\n",
    "    left_on = 'match_id',\n",
    "    right_on = 'full_match_report_url',\n",
    "    how = 'left'\n",
    ")[columns_select].drop_duplicates()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
