{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Pipeline - Production Guide\n",
    "\n",
    "This notebook demonstrates the complete data collection pipeline for the Pron√≥sticos Football project. It shows how to use the formal Python scripts to collect comprehensive football data from FBRef.\n",
    "\n",
    "## Pipeline Overview\n",
    "\n",
    "The data collection pipeline consists of 4 main steps:\n",
    "\n",
    "1. **Team ID Mapping** - Extract team IDs and create mappings\n",
    "2. **Fixtures Collection** - Collect match fixtures and results\n",
    "3. **Wages Collection** - Collect player wage information\n",
    "4. **Match Statistics Collection** - Collect detailed match statistics\n",
    "\n",
    "Each step builds upon the previous one, creating a comprehensive dataset for analysis and prediction modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import json\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Add scripts directory to path\n",
    "# Adjust this path based on notebook location:\n",
    "# From production/: '../..'\n",
    "# From production/data_collections/: '../../..'\n",
    "scripts_path = os.path.join('..', '..', 'scripts')  # Change to '../../../scripts' if moving to data_collections/\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "# Import our utilities\n",
    "from utils.data_utils import load_teams_from_json, fixtures_data_to_dataframe\n",
    "from utils.scraping_utils import get_page\n",
    "\n",
    "print(\"‚úÖ Setup complete - Ready to run data collection pipeline\")\n",
    "print(f\"üìÅ Working directory: {os.getcwd()}\")\n",
    "print(f\"üêç Python version: {sys.version}\")\n",
    "print(f\"üìÇ Scripts path: {os.path.abspath(scripts_path)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration Parameters\n",
    "\n",
    "Define the parameters for your data collection run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration-Based Approach (Recommended)\n",
    "# Load configuration from YAML files instead of defining parameters here\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add scripts path for configuration utilities\n",
    "# Adjust this path based on notebook location:\n",
    "# From production/: '../..'\n",
    "# From production/data_collections/: '../../..'\n",
    "scripts_path = os.path.join('..', '..', 'scripts')  # Change to '../../../scripts' if moving to data_collections/\n",
    "if scripts_path not in sys.path:\n",
    "    sys.path.append(scripts_path)\n",
    "\n",
    "from utils.config_utils import load_config\n",
    "\n",
    "# Choose your configuration\n",
    "CONFIG_NAME = 'prod'  # Options: 'prod', 'dev', 'testing', or path to custom config\n",
    "\n",
    "# Load the configuration\n",
    "config = load_config(CONFIG_NAME)\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"üöÄ Using Configuration-Based Data Collection\")\n",
    "config.print_summary()\n",
    "\n",
    "# Ensure data directories exist\n",
    "config.ensure_data_directories()\n",
    "print(f\"‚úÖ Data directories ensured for environment: {config.environment}\")\n",
    "\n",
    "# Legacy approach (deprecated but still available)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"LEGACY APPROACH (for reference):\")\n",
    "print(\"You can still define parameters manually if needed:\")\n",
    "\n",
    "# Configuration\n",
    "SEASONS = config.seasons\n",
    "OUTPUT_FORMATS = config.output_formats\n",
    "ENVIRONMENT = config.environment\n",
    "DATA_DIR = config.get_raw_data_path()\n",
    "# Adjust this path based on notebook location:\n",
    "SCRIPTS_DIR = '../../scripts/data_collection/'  # Change to '../../../scripts/data_collection/' if moving to data_collections/\n",
    "\n",
    "print(f\"   Seasons: {SEASONS}\")\n",
    "print(f\"   Formats: {OUTPUT_FORMATS}\")\n",
    "print(f\"   Environment: {ENVIRONMENT}\")\n",
    "print(f\"   Data directory: {DATA_DIR}\")\n",
    "print(\"\\nüí° To switch configurations, change CONFIG_NAME above to 'dev' or 'testing'\")\n",
    "print(f\"üìÇ Scripts directory: {os.path.abspath(SCRIPTS_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "Utility functions to run scripts and track progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions loaded\n"
     ]
    }
   ],
   "source": [
    "def run_script(script_name, args=None, capture_output=True):\n",
    "    \"\"\"\n",
    "    Run a data collection script and return the result.\n",
    "    \"\"\"\n",
    "    script_path = os.path.join(SCRIPTS_DIR, script_name)\n",
    "    cmd = [sys.executable, script_path]\n",
    "    \n",
    "    if args:\n",
    "        cmd.extend(args)\n",
    "    \n",
    "    print(f\"üöÄ Running: {' '.join(cmd)}\")\n",
    "    print(f\"‚è∞ Start time: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = subprocess.run(cmd, capture_output=capture_output, text=True, check=True)\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        print(f\"‚úÖ Script completed successfully in {duration:.1f}s\")\n",
    "        if capture_output and result.stdout:\n",
    "            print(\"üìã Output:\")\n",
    "            print(result.stdout)\n",
    "        \n",
    "        return True, result\n",
    "    \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"‚ùå Script failed after {duration:.1f}s with return code {e.returncode}\")\n",
    "        if capture_output and e.stderr:\n",
    "            print(\"üö® Error output:\")\n",
    "            print(e.stderr)\n",
    "        return False, e\n",
    "    \n",
    "    except Exception as e:\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"‚ùå Unexpected error after {duration:.1f}s: {e}\")\n",
    "        return False, e\n",
    "\n",
    "\n",
    "def check_file_exists(filepath, description=\"\"):\n",
    "    \"\"\"\n",
    "    Check if a file exists and show file info.\n",
    "    \"\"\"\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        modified = datetime.fromtimestamp(os.path.getmtime(filepath))\n",
    "        print(f\"‚úÖ {description}found: {filepath}\")\n",
    "        print(f\"   üìè Size: {size:,} bytes\")\n",
    "        print(f\"   üìÖ Modified: {modified.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"‚ùå {description}not found: {filepath}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def show_data_summary(filepath, data_type=\"\"):\n",
    "    \"\"\"\n",
    "    Show a summary of collected data.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filepath):\n",
    "        print(f\"‚ùå Cannot show summary - file not found: {filepath}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        if filepath.endswith('.json'):\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                print(f\"üìä {data_type}Summary:\")\n",
    "                print(f\"   üìÅ File: {os.path.basename(filepath)}\")\n",
    "                print(f\"   üóÇÔ∏è  Main keys: {len(data)} items\")\n",
    "                \n",
    "                if data_type.lower() == \"teams\":\n",
    "                    total_seasons = sum(len(team_data.get('seasons', [])) for team_data in data.values())\n",
    "                    print(f\"   üèüÔ∏è  Teams: {len(data)}\")\n",
    "                    print(f\"   üìÖ Total team-seasons: {total_seasons}\")\n",
    "                \n",
    "                elif isinstance(data, list):\n",
    "                    print(f\"   üìä Records: {len(data)}\")\n",
    "                    if data and isinstance(data[0], dict):\n",
    "                        print(f\"   üîë Sample keys: {list(data[0].keys())[:5]}\")\n",
    "        \n",
    "        elif filepath.endswith('.csv'):\n",
    "            df = pd.read_csv(filepath)\n",
    "            print(f\"üìä {data_type}Summary:\")\n",
    "            print(f\"   üìÅ File: {os.path.basename(filepath)}\")\n",
    "            print(f\"   üìä Shape: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "            print(f\"   üîë Columns: {list(df.columns)[:5]}{'...' if len(df.columns) > 5 else ''}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error reading file {filepath}: {e}\")\n",
    "\n",
    "print(\"‚úÖ Helper functions loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 1: Team ID Mapping\n",
    "\n",
    "The first step extracts team IDs from FBRef for all Premier League teams across the specified seasons. This creates the foundation mapping needed for all subsequent data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  STEP 1: TEAM ID MAPPING\n",
      "==================================================\n",
      "Using configuration-based script...\n",
      "üöÄ Running: c:\\Users\\50230\\anaconda3\\python.exe ../../scripts/data_collection/team_id_mapper_config.py --config prod\n",
      "‚è∞ Start time: 19:53:06\n",
      "‚úÖ Script completed successfully in 26.0s\n",
      "üìã Output:\n",
      "Configuration Summary:\n",
      "   Config file: c:\\Users\\50230\\OneDrive\\Escritorio\\Proyectos y trabajos\\Personales\\Pron√≥sticos Football\\config\\prod.yaml\n",
      "   Environment: prod\n",
      "   Seasons: ['2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
      "   Output formats: ['json', 'csv']\n",
      "   Teams filter: All teams\n",
      "   Competitions: ['Premier League']\n",
      "   Max matches: No limit\n",
      "   Enhanced scraper: True\n",
      "   Log level: INFO\n",
      "   Enabled steps: ['team_mapping', 'fixtures', 'wages', 'match_stats']\n",
      "Data saved to ../../data\\prod\\raw\\all_teams.json\n",
      "\n",
      "Successfully extracted 27 teams\n",
      "Seasons processed: 6\n",
      "Environment: prod\n",
      "Output saved to: ../../data\\prod\\raw\\all_teams.json\n",
      "\n",
      "‚úÖ Teams file found: ../../data\\prod\\raw\\all_teams.json\n",
      "   üìè Size: 6,546 bytes\n",
      "   üìÖ Modified: 2025-07-28 19:53:32\n",
      "üìä Teams Summary:\n",
      "   üìÅ File: all_teams.json\n",
      "   üóÇÔ∏è  Main keys: 27 items\n",
      "\n",
      "üìã Sample teams:\n",
      "   1. Arsenal (ID: 18bb7c10)\n",
      "      Seasons: ['2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
      "   2. Aston Villa (ID: 8602292d)\n",
      "      Seasons: ['2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
      "   3. Bournemouth (ID: 4ba7cbea)\n",
      "      Seasons: ['2019-2020', '2022-2023', '2023-2024', '2024-2025']\n",
      "\n",
      "==================================================\n",
      "ALTERNATIVE: Run complete pipeline with single command\n",
      "python run_all_collectors_config.py --config prod\n",
      "python run_all_collectors_config.py --config dev\n",
      "python run_all_collectors_config.py --config testing --dry-run\n"
     ]
    }
   ],
   "source": [
    "print(\"üèóÔ∏è  STEP 1: TEAM ID MAPPING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# RECOMMENDED: Configuration-based approach\n",
    "print(\"Using configuration-based script...\")\n",
    "\n",
    "# Run with configuration\n",
    "success, result = run_script('team_id_mapper_config.py', ['--config', CONFIG_NAME])\n",
    "\n",
    "if success:\n",
    "    # Check the output file\n",
    "    teams_file = config.get_raw_data_path('all_teams.json')\n",
    "    check_file_exists(teams_file, \"Teams file \")\n",
    "    show_data_summary(teams_file, \"Teams \")\n",
    "    \n",
    "    # Load and preview the data\n",
    "    try:\n",
    "        teams_data = load_teams_from_json(teams_file)\n",
    "        print(f\"\\nüìã Sample teams:\")\n",
    "        for i, (team_id, team_info) in enumerate(list(teams_data.items())[:3]):\n",
    "            print(f\"   {i+1}. {team_info['team_name']} (ID: {team_id})\")\n",
    "            print(f\"      Seasons: {team_info['seasons']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading teams data: {e}\")\n",
    "else:\n",
    "    print(\"‚ùå Step 1 failed - cannot proceed with pipeline\")\n",
    "    print(\"üí° Check the error messages above and ensure FBRef is accessible\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ALTERNATIVE: Run complete pipeline with single command\")\n",
    "print(\"python run_all_collectors_config.py --config prod\")\n",
    "print(\"python run_all_collectors_config.py --config dev\")\n",
    "print(\"python run_all_collectors_config.py --config testing --dry-run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 2: Fixtures Collection\n",
    "\n",
    "This step collects match fixtures and results for all teams. It creates a comprehensive dataset of all matches played by Premier League teams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  STEP 2: FIXTURES COLLECTION\n",
      "==================================================\n",
      "Using configuration-based script...\n",
      "üöÄ Running: c:\\Users\\50230\\anaconda3\\python.exe ../../scripts/data_collection/fixtures_collector_config.py --config prod\n",
      "‚è∞ Start time: 19:53:43\n",
      "‚úÖ Script completed successfully in 1049.4s\n",
      "üìã Output:\n",
      "Configuration Summary:\n",
      "   Config file: c:\\Users\\50230\\OneDrive\\Escritorio\\Proyectos y trabajos\\Personales\\Pron√≥sticos Football\\config\\prod.yaml\n",
      "   Environment: prod\n",
      "   Seasons: ['2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
      "   Output formats: ['json', 'csv']\n",
      "   Teams filter: All teams\n",
      "   Competitions: ['Premier League']\n",
      "   Max matches: No limit\n",
      "   Enhanced scraper: True\n",
      "   Log level: INFO\n",
      "   Enabled steps: ['team_mapping', 'fixtures', 'wages', 'match_stats']\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_10.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_20.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_30.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_40.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_50.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_60.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_70.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_80.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_90.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_100.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_110.json\n",
      "Data saved to ../../data\\prod\\raw\\fixtures_progress_120.json\n",
      "Data saved to ../../data\\prod\\raw\\all_competitions_fixtures.json\n",
      "DataFrame saved to ../../data\\prod\\raw\\all_competitions_fixtures_dataframe.json\n",
      "DataFrame saved to ../../data\\prod\\raw\\all_competitions_fixtures_dataframe.csv\n",
      "\n",
      "Successfully extracted fixtures for 27 teams\n",
      "Total matches: 5751\n",
      "Environment: prod\n",
      "Raw data saved to: ../../data\\prod\\raw\\all_competitions_fixtures.json\n",
      "DataFrame saved to: ../../data\\prod\\raw\\all_competitions_fixtures_dataframe.json\n",
      "DataFrame saved to: ../../data\\prod\\raw\\all_competitions_fixtures_dataframe.csv\n",
      "\n",
      "‚úÖ Fixtures file found: ../../data\\prod\\raw\\all_competitions_fixtures.json\n",
      "   üìè Size: 7,334,359 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:11:12\n",
      "üìä Fixtures Summary:\n",
      "   üìÅ File: all_competitions_fixtures.json\n",
      "   üóÇÔ∏è  Main keys: 27 items\n",
      "‚úÖ Fixtures DataFrame (json) found: ../../data\\prod\\raw\\all_competitions_fixtures_dataframe.json\n",
      "   üìè Size: 7,355,393 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:11:12\n",
      "‚úÖ Fixtures DataFrame (csv) found: ../../data\\prod\\raw\\all_competitions_fixtures_dataframe.csv\n",
      "   üìè Size: 3,796,016 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:11:13\n",
      "üìä Fixtures DataFrame Summary:\n",
      "   üìÅ File: all_competitions_fixtures_dataframe.csv\n",
      "   üìä Shape: 5,751 rows √ó 30 columns\n",
      "   üîë Columns: ['team_name', 'season', 'team_id', 'date', 'comp']...\n",
      "\n",
      "üìã Fixtures DataFrame Preview:\n",
      "   Shape: (5751, 30)\n",
      "   Columns: ['team_name', 'season', 'team_id', 'date', 'comp', 'round', 'venue', 'result']...\n",
      "   Sample matches:\n",
      "   1. Arsenal vs Newcastle Utd (2019-08-11)\n",
      "   2. Arsenal vs Burnley (2019-08-17)\n",
      "   3. Arsenal vs Liverpool (2019-08-24)\n",
      "\n",
      "==================================================\n",
      "LEGACY APPROACH (for reference):\n",
      "The old approach required manual argument preparation:\n",
      "   Teams filter: All teams\n",
      "   Seasons filter: ['2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
      "   All parameters are now handled by the configuration file!\n"
     ]
    }
   ],
   "source": [
    "print(\"üèóÔ∏è  STEP 2: FIXTURES COLLECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# RECOMMENDED: Configuration-based approach\n",
    "print(\"Using configuration-based script...\")\n",
    "\n",
    "# Run with configuration\n",
    "success, result = run_script('fixtures_collector_config.py', ['--config', CONFIG_NAME])\n",
    "\n",
    "if success:\n",
    "    # Check output files\n",
    "    fixtures_file = config.get_raw_data_path('all_competitions_fixtures.json')\n",
    "    check_file_exists(fixtures_file, \"Fixtures file \")\n",
    "    show_data_summary(fixtures_file, \"Fixtures \")\n",
    "    \n",
    "    # Check DataFrame files\n",
    "    for fmt in config.output_formats:\n",
    "        df_file = config.get_raw_data_path(f'all_competitions_fixtures_dataframe.{fmt}')\n",
    "        if check_file_exists(df_file, f\"Fixtures DataFrame ({fmt}) \"):\n",
    "            if fmt == 'csv':\n",
    "                show_data_summary(df_file, \"Fixtures DataFrame \")\n",
    "    \n",
    "    # Preview the data\n",
    "    try:\n",
    "        with open(fixtures_file, 'r', encoding='utf-8') as f:\n",
    "            fixtures_data = json.load(f)\n",
    "        \n",
    "        fixtures_df = fixtures_data_to_dataframe(fixtures_data)\n",
    "        print(f\"\\nüìã Fixtures DataFrame Preview:\")\n",
    "        print(f\"   Shape: {fixtures_df.shape}\")\n",
    "        print(f\"   Columns: {list(fixtures_df.columns)[:8]}...\")\n",
    "        print(f\"   Sample matches:\")\n",
    "        for i in range(min(3, len(fixtures_df))):\n",
    "            row = fixtures_df.iloc[i]\n",
    "            print(f\"   {i+1}. {row['team_name']} vs {row.get('opponent', 'N/A')} ({row.get('date', 'N/A')})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error previewing fixtures data: {e}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Step 2 failed - check error messages above\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LEGACY APPROACH (for reference):\")\n",
    "print(\"The old approach required manual argument preparation:\")\n",
    "print(f\"   Teams filter: {config.get_effective_teams() or 'All teams'}\")\n",
    "print(f\"   Seasons filter: {config.get_effective_seasons()}\")\n",
    "print(\"   All parameters are now handled by the configuration file!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 3: Wages Collection\n",
    "\n",
    "This step collects player wage information for all teams. Note that wage data might not be available for all teams/seasons on FBRef."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  STEP 3: WAGES COLLECTION\n",
      "==================================================\n",
      "‚ö†Ô∏è  Note: Wage collection can take a long time and may have limited data availability\n",
      "üìä This step is optional for the overall pipeline\n",
      "Using configuration-based script...\n",
      "‚ö†Ô∏è  Configuration-based wages collector not yet implemented\n",
      "üìã This would run: wages_collector_config.py --config prod\n",
      "‚úÖ Teams file found: ../../data\\prod\\raw\\all_teams.json\n",
      "   üìè Size: 6,546 bytes\n",
      "   üìÖ Modified: 2025-07-28 19:53:32\n",
      "üöÄ Running: c:\\Users\\50230\\anaconda3\\python.exe ../../scripts/data_collection/wages_collector.py --environment prod --output-formats json csv --summary --log-level INFO\n",
      "‚è∞ Start time: 20:12:30\n",
      "‚úÖ Script completed successfully in 600.3s\n",
      "üìã Output:\n",
      "Data saved to ../../data/prod/raw\\wages_progress_10.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_20.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_30.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_40.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_50.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_60.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_70.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_80.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_90.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_100.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_110.json\n",
      "Data saved to ../../data/prod/raw\\wages_progress_120.json\n",
      "Data saved to ../../data/prod/raw/premier_league_wages.json\n",
      "DataFrame saved to ../../data/prod/raw/premier_league_wages_dataframe.json\n",
      "DataFrame saved to ../../data/prod/raw/premier_league_wages_dataframe.csv\n",
      "Data saved to ../../data/prod/raw/premier_league_wages_summary.json\n",
      "\n",
      "üìä Wage Collection Summary:\n",
      "   Teams processed: 27\n",
      "   Total seasons: 120\n",
      "   Total players: 3341\n",
      "   Average players per team: 123.7\n",
      "\n",
      "‚úÖ Successfully extracted wages for 27 teams\n",
      "üìä Total player records: 3341\n",
      "üåç Environment: prod\n",
      "üìÅ Raw data saved to: ../../data/prod/raw/premier_league_wages.json\n",
      "üìÅ DataFrame saved to: ../../data/prod/raw/premier_league_wages_dataframe.json\n",
      "üìÅ DataFrame saved to: ../../data/prod/raw/premier_league_wages_dataframe.csv\n",
      "\n",
      "‚úÖ Wages file found: ../../data\\prod\\raw\\premier_league_wages.json\n",
      "   üìè Size: 1,208,728 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:22:30\n",
      "üìä Wages Summary:\n",
      "   üìÅ File: premier_league_wages.json\n",
      "   üóÇÔ∏è  Main keys: 27 items\n",
      "‚úÖ Wages summary found: ../../data\\prod\\raw\\premier_league_wages_summary.json\n",
      "   üìè Size: 982 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:22:30\n",
      "\n",
      "üìä Wages Collection Summary:\n",
      "   Teams processed: 27\n",
      "   Total seasons: 120\n",
      "   Total players: 3341\n",
      "‚úÖ Wages DataFrame (json) found: ../../data\\prod\\raw\\premier_league_wages_dataframe.json\n",
      "   üìè Size: 1,342,755 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:22:30\n",
      "‚úÖ Wages DataFrame (csv) found: ../../data\\prod\\raw\\premier_league_wages_dataframe.csv\n",
      "   üìè Size: 518,331 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:22:30\n"
     ]
    }
   ],
   "source": [
    "print(\"üèóÔ∏è  STEP 3: WAGES COLLECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if wages collection is enabled\n",
    "if not config.is_step_enabled('wages'):\n",
    "    print(\"‚ö†Ô∏è  Wages collection is disabled in the current configuration\")\n",
    "    print(f\"üìã To enable it, set steps.wages.enabled=true in {CONFIG_NAME}.yaml\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Note: Wage collection can take a long time and may have limited data availability\")\n",
    "    print(\"üìä This step is optional for the overall pipeline\")\n",
    "    \n",
    "    # RECOMMENDED: Configuration-based approach\n",
    "    print(\"Using configuration-based script...\")\n",
    "    \n",
    "    # Note: wages_collector_config.py not created yet - would need to be implemented\n",
    "    # For now, show what would happen\n",
    "    print(\"‚ö†Ô∏è  Configuration-based wages collector not yet implemented\")\n",
    "    print(\"üìã This would run: wages_collector_config.py --config\", CONFIG_NAME)\n",
    "    \n",
    "    # Legacy approach for now\n",
    "    teams_file = config.get_raw_data_path('all_teams.json')\n",
    "    if check_file_exists(teams_file, \"Teams file \"):\n",
    "        wages_args = [\n",
    "            '--environment', config.environment,\n",
    "            '--output-formats'] + config.output_formats + [\n",
    "            '--summary',\n",
    "            '--log-level', config.log_level\n",
    "        ]\n",
    "        \n",
    "        # Add filters from configuration\n",
    "        if config.get_effective_teams():\n",
    "            wages_args.extend(['--teams'] + config.get_effective_teams())\n",
    "        \n",
    "        effective_seasons = config.get_effective_seasons()\n",
    "        if effective_seasons != config.seasons:\n",
    "            wages_args.extend(['--seasons'] + effective_seasons)\n",
    "        \n",
    "        # Run wages collection (legacy script)\n",
    "        success, result = run_script('wages_collector.py', wages_args)\n",
    "        \n",
    "        if success:\n",
    "            # Check output files\n",
    "            wages_file = config.get_raw_data_path('premier_league_wages.json')\n",
    "            check_file_exists(wages_file, \"Wages file \")\n",
    "            show_data_summary(wages_file, \"Wages \")\n",
    "            \n",
    "            # Check summary file\n",
    "            summary_file = config.get_raw_data_path('premier_league_wages_summary.json')\n",
    "            if check_file_exists(summary_file, \"Wages summary \"):\n",
    "                try:\n",
    "                    with open(summary_file, 'r', encoding='utf-8') as f:\n",
    "                        summary = json.load(f)\n",
    "                    print(f\"\\nüìä Wages Collection Summary:\")\n",
    "                    print(f\"   Teams processed: {summary.get('total_teams', 'N/A')}\")\n",
    "                    print(f\"   Total seasons: {summary.get('total_seasons', 'N/A')}\")\n",
    "                    print(f\"   Total players: {summary.get('total_players', 'N/A')}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error reading wages summary: {e}\")\n",
    "            \n",
    "            # Check DataFrame files\n",
    "            for fmt in config.output_formats:\n",
    "                df_file = config.get_raw_data_path(f'premier_league_wages_dataframe.{fmt}')\n",
    "                check_file_exists(df_file, f\"Wages DataFrame ({fmt}) \")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  Step 3 failed - this is common due to limited wage data availability\")\n",
    "            print(\"üìã The pipeline can continue without wage data\")\n",
    "    else:\n",
    "        print(\"‚ùå Cannot proceed - teams file not found\")\n",
    "        print(\"üí° Please run Step 1 first\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Step 4: Match Statistics Collection\n",
    "\n",
    "This is the most comprehensive step, collecting detailed match statistics for all matches. It uses enhanced scraping with anti-blocking measures and can take several hours for the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è  STEP 4: MATCH STATISTICS COLLECTION\n",
      "==================================================\n",
      "‚úÖ Fixtures file found: ../../data\\prod\\raw\\all_competitions_fixtures.json\n",
      "   üìè Size: 7,334,359 bytes\n",
      "   üìÖ Modified: 2025-07-28 20:11:12\n",
      "üìä Scope Analysis (based on configuration):\n",
      "   Total fixtures: 5,751\n",
      "   After configuration filters: 4,560\n",
      "   Unique matches to process: 2,280\n",
      "   Teams filter: All teams\n",
      "   Competitions: ['Premier League']\n",
      "   ‚è∞ Estimated time: 12.7 hours\n",
      "\n",
      "‚ö†Ô∏è  Important Notes:\n",
      "   - This step can take several hours for the complete dataset\n",
      "   - Uses enhanced scraping with rate limiting\n",
      "   - Progress is saved every 20 matches\n",
      "   - Can be resumed if interrupted\n",
      "\n",
      "üîî About to process 2,280 matches (estimated 12.7 hours)\n",
      "üí° Consider using 'testing' config for initial testing (limited to 10 matches)\n",
      "\n",
      "Using configuration-based script...\n",
      "‚ö†Ô∏è  Configuration-based match stats collector not yet implemented\n",
      "üìã This would run: match_stats_collector_config.py --config prod\n",
      "\n",
      "üöÄ Starting match statistics collection...\n",
      "üöÄ Running: c:\\Users\\50230\\anaconda3\\python.exe ../../scripts/data_collection/match_stats_collector.py --environment prod --output-formats json csv --competitions Premier League --enhanced-scraper --log-level INFO\n",
      "‚è∞ Start time: 20:23:42\n"
     ]
    }
   ],
   "source": [
    "print(\"üèóÔ∏è  STEP 4: MATCH STATISTICS COLLECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if step is enabled\n",
    "if not config.is_step_enabled('match_stats'):\n",
    "    print(\"‚ö†Ô∏è  Match statistics collection is disabled in the current configuration\")\n",
    "    print(f\"üìã To enable it, set steps.match_stats.enabled=true in {CONFIG_NAME}.yaml\")\n",
    "else:\n",
    "    # Check if we have fixtures data\n",
    "    fixtures_file = config.get_raw_data_path('all_competitions_fixtures.json')\n",
    "    if not check_file_exists(fixtures_file, \"Fixtures file \"):\n",
    "        print(\"‚ùå Cannot proceed - fixtures file not found\")\n",
    "        print(\"üí° Please run Step 2 first\")\n",
    "    else:\n",
    "        # Estimate the scope using configuration parameters\n",
    "        try:\n",
    "            with open(fixtures_file, 'r', encoding='utf-8') as f:\n",
    "                fixtures_data = json.load(f)\n",
    "            fixtures_df = fixtures_data_to_dataframe(fixtures_data)\n",
    "            \n",
    "            # Apply filters from configuration\n",
    "            filtered_df = fixtures_df.copy()\n",
    "            \n",
    "            # Apply team filter if specified\n",
    "            if config.get_effective_teams():\n",
    "                filtered_df = filtered_df[filtered_df['team_name'].isin(config.get_effective_teams())]\n",
    "            \n",
    "            # Apply season filter if specified\n",
    "            effective_seasons = config.get_effective_seasons()\n",
    "            if effective_seasons != config.seasons:\n",
    "                filtered_df = filtered_df[filtered_df['season'].isin(effective_seasons)]\n",
    "            \n",
    "            # Apply competition filter\n",
    "            filtered_df = filtered_df[filtered_df['comp'].isin(config.competitions_filter)]\n",
    "            \n",
    "            unique_matches = filtered_df['match_report_href'].nunique() if 'match_report_href' in filtered_df.columns else len(filtered_df)\n",
    "            \n",
    "            print(f\"üìä Scope Analysis (based on configuration):\")\n",
    "            print(f\"   Total fixtures: {len(fixtures_df):,}\")\n",
    "            print(f\"   After configuration filters: {len(filtered_df):,}\")\n",
    "            print(f\"   Unique matches to process: {unique_matches:,}\")\n",
    "            print(f\"   Teams filter: {config.get_effective_teams() or 'All teams'}\")\n",
    "            print(f\"   Competitions: {config.competitions_filter}\")\n",
    "            \n",
    "            if config.max_matches:\n",
    "                print(f\"   Limited to: {config.max_matches} matches (from configuration)\")\n",
    "                unique_matches = min(unique_matches, config.max_matches)\n",
    "            \n",
    "            # Time estimation\n",
    "            estimated_hours = unique_matches * 20 / 3600  # ~20 seconds per match\n",
    "            print(f\"   ‚è∞ Estimated time: {estimated_hours:.1f} hours\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error analyzing scope: {e}\")\n",
    "        \n",
    "        print(\"\\n‚ö†Ô∏è  Important Notes:\")\n",
    "        print(\"   - This step can take several hours for the complete dataset\")\n",
    "        print(\"   - Uses enhanced scraping with rate limiting\")\n",
    "        print(\"   - Progress is saved every 20 matches\")\n",
    "        print(\"   - Can be resumed if interrupted\")\n",
    "        \n",
    "        # Ask for confirmation if processing many matches\n",
    "        if not config.max_matches and unique_matches > 100:\n",
    "            print(f\"\\nüîî About to process {unique_matches:,} matches (estimated {estimated_hours:.1f} hours)\")\n",
    "            print(\"üí° Consider using 'testing' config for initial testing (limited to 10 matches)\")\n",
    "        \n",
    "        # RECOMMENDED: Configuration-based approach\n",
    "        print(\"\\nUsing configuration-based script...\")\n",
    "        \n",
    "        # Note: match_stats_collector_config.py not created yet - would need to be implemented\n",
    "        print(\"‚ö†Ô∏è  Configuration-based match stats collector not yet implemented\")\n",
    "        print(\"üìã This would run: match_stats_collector_config.py --config\", CONFIG_NAME)\n",
    "        \n",
    "        # Legacy approach for now\n",
    "        stats_args = [\n",
    "            '--environment', config.environment,\n",
    "            '--output-formats'] + config.output_formats + [\n",
    "            '--competitions'] + config.competitions_filter + [\n",
    "            '--enhanced-scraper' if config.enhanced_scraper else '--no-enhanced-scraper',\n",
    "            '--log-level', config.log_level\n",
    "        ]\n",
    "        \n",
    "        # Add filters from configuration\n",
    "        if config.get_effective_teams():\n",
    "            stats_args.extend(['--teams'] + config.get_effective_teams())\n",
    "        \n",
    "        effective_seasons = config.get_effective_seasons()\n",
    "        if effective_seasons != config.seasons:\n",
    "            stats_args.extend(['--seasons'] + effective_seasons)\n",
    "        \n",
    "        if config.max_matches:\n",
    "            stats_args.extend(['--max-matches', str(config.max_matches)])\n",
    "        \n",
    "        # Run match statistics collection\n",
    "        print(f\"\\nüöÄ Starting match statistics collection...\")\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        success, result = run_script('match_stats_collector.py', stats_args, capture_output=False)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f\"\\n‚è∞ Collection completed in: {duration}\")\n",
    "        \n",
    "        if success:\n",
    "            # Check output files\n",
    "            stats_file = config.get_raw_data_path('match_stats', 'all_match_stats.json')\n",
    "            check_file_exists(stats_file, \"Match stats file \")\n",
    "            \n",
    "            # Check DataFrame files\n",
    "            for fmt in config.output_formats:\n",
    "                df_file = config.get_raw_data_path('match_stats', f'all_match_stats_dataframe.{fmt}')\n",
    "                if check_file_exists(df_file, f\"Match stats DataFrame ({fmt}) \"):\n",
    "                    if fmt == 'csv':\n",
    "                        show_data_summary(df_file, \"Match Stats DataFrame \")\n",
    "            \n",
    "            # Preview the data\n",
    "            if os.path.exists(stats_file):\n",
    "                try:\n",
    "                    with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "                        stats_data = json.load(f)\n",
    "                    \n",
    "                    if stats_data:\n",
    "                        print(f\"\\nüìã Match Statistics Preview:\")\n",
    "                        print(f\"   Total records: {len(stats_data):,}\")\n",
    "                        \n",
    "                        # Show unique matches and stats\n",
    "                        unique_matches = len(set(record['match_id'] for record in stats_data))\n",
    "                        unique_stats = len(set(record['stat_name'] for record in stats_data))\n",
    "                        print(f\"   Unique matches: {unique_matches:,}\")\n",
    "                        print(f\"   Unique statistics: {unique_stats}\")\n",
    "                        \n",
    "                        # Sample statistics\n",
    "                        sample_stats = sorted(set(record['stat_name'] for record in stats_data[:100]))\n",
    "                        print(f\"   Sample stats: {sample_stats[:5]}...\")\n",
    "                except Exception as e:\n",
    "                    print(f\"‚ùå Error previewing match stats: {e}\")\n",
    "        \n",
    "        else:\n",
    "            print(\"‚ùå Step 4 failed - check error messages above\")\n",
    "            print(\"üí° Match statistics collection can be resumed using existing progress files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Pipeline Summary and Data Overview\n",
    "\n",
    "Review the complete dataset collected through the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä PIPELINE SUMMARY\n",
      "==================================================\n",
      "‚úÖ Teams Mapping: 6,546 bytes (0.0 MB)\n",
      "‚úÖ Fixtures: 7,334,359 bytes (7.0 MB)\n",
      "‚úÖ Wages: 1,208,728 bytes (1.2 MB)\n",
      "‚úÖ Match Statistics: 20,331,565 bytes (19.4 MB)\n",
      "\n",
      "üìà Total data collected: 28,881,198 bytes (27.5 MB)\n",
      "\n",
      "üéâ Pipeline completed successfully!\n",
      "üìÅ All enabled datasets have been collected\n",
      "üöÄ Ready for data engineering and analysis phase\n",
      "\n",
      "üìã Configuration used: prod\n",
      "   Environment: prod\n",
      "   Data directory: ../../data\\prod\\raw\n",
      "   Available formats: ['json', 'csv']\n",
      "   JSON: 7 files\n",
      "   CSV: 2 files\n"
     ]
    }
   ],
   "source": [
    "print(\"üìä PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check all major output files using configuration paths\n",
    "output_files = {\n",
    "    'Teams Mapping': config.get_raw_data_path('all_teams.json'),\n",
    "    'Fixtures': config.get_raw_data_path('all_competitions_fixtures.json'),\n",
    "    'Wages': config.get_raw_data_path('premier_league_wages.json'),\n",
    "    'Match Statistics': config.get_raw_data_path('match_stats', 'all_match_stats.json')\n",
    "}\n",
    "\n",
    "pipeline_success = True\n",
    "file_sizes = {}\n",
    "\n",
    "for description, filepath in output_files.items():\n",
    "    if os.path.exists(filepath):\n",
    "        size = os.path.getsize(filepath)\n",
    "        file_sizes[description] = size\n",
    "        print(f\"‚úÖ {description}: {size:,} bytes ({size/1024/1024:.1f} MB)\")\n",
    "    else:\n",
    "        print(f\"‚ùå {description}: Not found\")\n",
    "        # Only consider it a failure if the step is enabled in configuration\n",
    "        step_name = {\n",
    "            'Teams Mapping': 'team_mapping',\n",
    "            'Fixtures': 'fixtures', \n",
    "            'Wages': 'wages',\n",
    "            'Match Statistics': 'match_stats'\n",
    "        }.get(description)\n",
    "        \n",
    "        if step_name and config.is_step_enabled(step_name) and description != 'Wages':\n",
    "            pipeline_success = False\n",
    "\n",
    "print(f\"\\nüìà Total data collected: {sum(file_sizes.values()):,} bytes ({sum(file_sizes.values())/1024/1024:.1f} MB)\")\n",
    "\n",
    "if pipeline_success:\n",
    "    print(\"\\nüéâ Pipeline completed successfully!\")\n",
    "    print(\"üìÅ All enabled datasets have been collected\")\n",
    "    print(\"üöÄ Ready for data engineering and analysis phase\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Pipeline completed with some missing components\")\n",
    "    print(\"üí° Review the error messages above to address any issues\")\n",
    "\n",
    "# Show configuration summary\n",
    "print(f\"\\nüìã Configuration used: {CONFIG_NAME}\")\n",
    "print(f\"   Environment: {config.environment}\")\n",
    "print(f\"   Data directory: {config.get_raw_data_path()}\")\n",
    "print(f\"   Available formats: {config.output_formats}\")\n",
    "\n",
    "# Count files by format\n",
    "for fmt in config.output_formats:\n",
    "    format_files = []\n",
    "    data_dir = config.get_raw_data_path()\n",
    "    for root, dirs, files in os.walk(data_dir):\n",
    "        format_files.extend([f for f in files if f.endswith(f'.{fmt}')])\n",
    "    print(f\"   {fmt.upper()}: {len(format_files)} files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Quick Data Exploration\n",
    "\n",
    "Let's explore the collected data to understand what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç QUICK DATA EXPLORATION\n",
      "==================================================\n",
      "\n",
      "üèüÔ∏è  TEAMS DATA:\n",
      "   Total teams: 27\n",
      "   Teams by season:\n",
      "     2019-2020: 20 teams\n",
      "     2020-2021: 20 teams\n",
      "     2021-2022: 20 teams\n",
      "     2022-2023: 20 teams\n",
      "     2023-2024: 20 teams\n",
      "     2024-2025: 20 teams\n",
      "\n",
      "‚öΩ FIXTURES DATA:\n",
      "   Total fixtures: 5,751\n",
      "   By competition:\n",
      "     Premier League: 4,560 matches\n",
      "     FA Cup: 362 matches\n",
      "     EFL Cup: 341 matches\n",
      "     Champions Lg: 235 matches\n",
      "     Europa Lg: 177 matches\n",
      "     Conf Lg: 60 matches\n",
      "     Community Shield: 10 matches\n",
      "     Super Cup: 4 matches\n",
      "     FA Community Shield: 2 matches\n",
      "   By season:\n",
      "     2019-2020: 957 matches\n",
      "     2020-2021: 960 matches\n",
      "     2021-2022: 955 matches\n",
      "     2022-2023: 948 matches\n",
      "     2023-2024: 956 matches\n",
      "     2024-2025: 975 matches\n",
      "\n",
      "üìä MATCH STATISTICS DATA:\n",
      "   Total records: 98,164\n",
      "   Unique matches: 3,236\n",
      "   Available statistics (16):\n",
      "     Basic: ['Passing Accuracy', 'Possession', 'Saves', 'Shots on Target']\n",
      "     Advanced: ['Aerials Won', 'Clearances', 'Corners', 'Crosses', 'Fouls', 'Goal Kicks', 'Interceptions', 'Long Balls', 'Offsides', 'Tackles']...\n",
      "\n",
      "‚úÖ Data exploration complete for prod environment!\n",
      "üìÅ Data location: ../../data\\prod\\raw\n",
      "\n",
      "üìã Configuration filters applied:\n",
      "   Teams: All teams\n",
      "   Seasons: ['2019-2020', '2020-2021', '2021-2022', '2022-2023', '2023-2024', '2024-2025']\n",
      "   Competitions: ['Premier League']\n",
      "   Max matches: No limit\n"
     ]
    }
   ],
   "source": [
    "print(\"üîç QUICK DATA EXPLORATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Load and explore teams data\n",
    "teams_file = config.get_raw_data_path('all_teams.json')\n",
    "if os.path.exists(teams_file):\n",
    "    try:\n",
    "        teams_data = load_teams_from_json(teams_file)\n",
    "        print(f\"\\nüèüÔ∏è  TEAMS DATA:\")\n",
    "        print(f\"   Total teams: {len(teams_data)}\")\n",
    "        \n",
    "        # Count by seasons\n",
    "        season_counts = {}\n",
    "        for team_data in teams_data.values():\n",
    "            for season in team_data.get('seasons', []):\n",
    "                season_counts[season] = season_counts.get(season, 0) + 1\n",
    "        \n",
    "        print(f\"   Teams by season:\")\n",
    "        for season, count in sorted(season_counts.items()):\n",
    "            print(f\"     {season}: {count} teams\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exploring teams data: {e}\")\n",
    "\n",
    "# Load and explore fixtures data\n",
    "fixtures_file = config.get_raw_data_path('all_competitions_fixtures.json')\n",
    "if os.path.exists(fixtures_file):\n",
    "    try:\n",
    "        with open(fixtures_file, 'r', encoding='utf-8') as f:\n",
    "            fixtures_data = json.load(f)\n",
    "        \n",
    "        fixtures_df = fixtures_data_to_dataframe(fixtures_data)\n",
    "        print(f\"\\n‚öΩ FIXTURES DATA:\")\n",
    "        print(f\"   Total fixtures: {len(fixtures_df):,}\")\n",
    "        \n",
    "        # By competition\n",
    "        if 'comp' in fixtures_df.columns:\n",
    "            comp_counts = fixtures_df['comp'].value_counts()\n",
    "            print(f\"   By competition:\")\n",
    "            for comp, count in comp_counts.items():\n",
    "                print(f\"     {comp}: {count:,} matches\")\n",
    "        \n",
    "        # By season\n",
    "        if 'season' in fixtures_df.columns:\n",
    "            season_counts = fixtures_df['season'].value_counts().sort_index()\n",
    "            print(f\"   By season:\")\n",
    "            for season, count in season_counts.items():\n",
    "                print(f\"     {season}: {count:,} matches\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exploring fixtures data: {e}\")\n",
    "\n",
    "# Load and explore match stats data\n",
    "stats_file = config.get_raw_data_path('match_stats', 'all_match_stats.json')\n",
    "if os.path.exists(stats_file):\n",
    "    try:\n",
    "        with open(stats_file, 'r', encoding='utf-8') as f:\n",
    "            stats_data = json.load(f)\n",
    "        \n",
    "        print(f\"\\nüìä MATCH STATISTICS DATA:\")\n",
    "        print(f\"   Total records: {len(stats_data):,}\")\n",
    "        \n",
    "        if stats_data:\n",
    "            # Unique matches and statistics\n",
    "            unique_matches = len(set(record['match_id'] for record in stats_data))\n",
    "            unique_stats = sorted(set(record['stat_name'] for record in stats_data))\n",
    "            \n",
    "            print(f\"   Unique matches: {unique_matches:,}\")\n",
    "            print(f\"   Available statistics ({len(unique_stats)}):\")\n",
    "            \n",
    "            # Group stats by category\n",
    "            basic_stats = [s for s in unique_stats if any(word in s.lower() for word in ['possession', 'passing', 'shots', 'saves'])]\n",
    "            advanced_stats = [s for s in unique_stats if s not in basic_stats]\n",
    "            \n",
    "            print(f\"     Basic: {basic_stats}\")\n",
    "            print(f\"     Advanced: {advanced_stats[:10]}{'...' if len(advanced_stats) > 10 else ''}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error exploring match stats data: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Data exploration complete for {config.environment} environment!\")\n",
    "print(f\"üìÅ Data location: {config.get_raw_data_path()}\")\n",
    "\n",
    "# Show configuration filters applied\n",
    "print(f\"\\nüìã Configuration filters applied:\")\n",
    "print(f\"   Teams: {config.get_effective_teams() or 'All teams'}\")\n",
    "print(f\"   Seasons: {config.get_effective_seasons()}\")\n",
    "print(f\"   Competitions: {config.competitions_filter}\")\n",
    "print(f\"   Max matches: {config.max_matches or 'No limit'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Alternative: Run Complete Pipeline with Single Command\n",
    "\n",
    "You can also run the entire pipeline using the orchestrator script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üöÄ CONFIGURATION-BASED PIPELINE (RECOMMENDED)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"\\nThe pipeline now supports YAML configuration files for better organization:\")\n",
    "\n",
    "print(\"\\nüìã Quick Start:\")\n",
    "print(\"   # Production data collection\")\n",
    "print(\"   python run_all_collectors_config.py --config prod\")\n",
    "print(\"   \")\n",
    "print(\"   # Development (faster, limited data)\")  \n",
    "print(\"   python run_all_collectors_config.py --config dev\")\n",
    "print(\"   \")\n",
    "print(\"   # Quick testing (minimal data)\")\n",
    "print(\"   python run_all_collectors_config.py --config testing\")\n",
    "\n",
    "print(\"\\nüìã Individual steps:\")\n",
    "print(\"   python team_id_mapper_config.py --config prod\")\n",
    "print(\"   python fixtures_collector_config.py --config dev\")\n",
    "print(\"   python run_all_collectors_config.py --config prod --step fixtures\")\n",
    "\n",
    "print(\"\\nüìã Configuration options:\")\n",
    "print(\"   --config prod     # Complete production collection\")\n",
    "print(\"   --config dev      # Development with limited teams/seasons\")  \n",
    "print(\"   --config testing  # Quick validation (10 matches only)\")\n",
    "print(\"   --dry-run         # Show what would be done\")\n",
    "print(\"   --skip-existing   # Skip steps where output exists\")\n",
    "print(\"   --step STEP       # Run only specific step\")\n",
    "\n",
    "print(\"\\nüí° Configuration files are in config/ directory:\")\n",
    "print(\"   config/prod.yaml     # Production settings\")\n",
    "print(\"   config/dev.yaml      # Development settings\")\n",
    "print(\"   config/testing.yaml  # Quick testing settings\")\n",
    "\n",
    "print(\"\\nüéØ Benefits of configuration approach:\")\n",
    "print(\"   ‚úÖ All settings in one place\")\n",
    "print(\"   ‚úÖ Easy environment switching (dev/prod)\")\n",
    "print(\"   ‚úÖ No long command-line arguments\") \n",
    "print(\"   ‚úÖ Reusable configurations\")\n",
    "print(\"   ‚úÖ Documentation in config files\")\n",
    "print(\"   ‚úÖ Version control for parameters\")\n",
    "\n",
    "print(f\"\\nüìä Current configuration: {CONFIG_NAME}\")\n",
    "config.print_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Next Steps\n",
    "\n",
    "After completing the data collection pipeline, you can proceed to the data engineering phase:\n",
    "\n",
    "## üîÑ Data Engineering Phase\n",
    "- **Data Validation**: Verify data quality and completeness\n",
    "- **Data Transformation**: Clean and standardize the collected data\n",
    "- **Feature Engineering**: Create derived features for modeling\n",
    "- **Data Integration**: Combine different data sources\n",
    "\n",
    "## üìä Analysis Phase\n",
    "- **Exploratory Data Analysis**: Understand patterns and relationships\n",
    "- **Statistical Analysis**: Generate insights from the data\n",
    "- **Visualization**: Create charts and dashboards\n",
    "\n",
    "## ü§ñ Modeling Phase\n",
    "- **Feature Selection**: Choose relevant variables for prediction\n",
    "- **Model Training**: Build and train prediction models\n",
    "- **Model Evaluation**: Assess model performance\n",
    "- **Model Deployment**: Deploy models for production use\n",
    "\n",
    "## üìÅ Data Structure Summary\n",
    "\n",
    "The pipeline creates the following key datasets in your chosen environment:\n",
    "\n",
    "```\n",
    "data/\n",
    "‚îú‚îÄ‚îÄ dev/                                     # Development environment\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ raw/\n",
    "‚îî‚îÄ‚îÄ prod/                                    # Production environment\n",
    "    ‚îî‚îÄ‚îÄ raw/\n",
    "        ‚îú‚îÄ‚îÄ all_teams.json                          # Team ID mappings\n",
    "        ‚îú‚îÄ‚îÄ all_competitions_fixtures.json          # Match fixtures and results\n",
    "        ‚îú‚îÄ‚îÄ all_competitions_fixtures_dataframe.csv # Fixtures as DataFrame\n",
    "        ‚îú‚îÄ‚îÄ premier_league_wages.json               # Player wage data\n",
    "        ‚îú‚îÄ‚îÄ premier_league_wages_dataframe.csv      # Wages as DataFrame\n",
    "        ‚îî‚îÄ‚îÄ match_stats/\n",
    "            ‚îú‚îÄ‚îÄ all_match_stats.json                # Detailed match statistics\n",
    "            ‚îî‚îÄ‚îÄ all_match_stats_dataframe.csv       # Stats as DataFrame\n",
    "```\n",
    "\n",
    "### Environment Management\n",
    "- **dev/**: For development, testing, and experimentation\n",
    "- **prod/**: For production-ready, validated datasets\n",
    "- Change the `ENVIRONMENT` variable above to switch between environments\n",
    "\n",
    "Each dataset is available in multiple formats (JSON, CSV, Parquet) for flexibility in downstream processing.\n",
    "\n",
    "### Data Collection Scripts Integration\n",
    "All formal scripts are designed to work with this environment structure:\n",
    "- Use `--output-file` parameters to specify environment paths\n",
    "- Scripts automatically create necessary subdirectories\n",
    "- Progress files are saved in environment-specific locations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
